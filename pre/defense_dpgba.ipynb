{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_765952/3417948181.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/zbz5349/anaconda3/envs/spurious/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(cuda=True, dataset='ogbn-arxiv', debug=True, defense_mode='none', device_id=2, dis_weight=1, dropout=0.5, epochs=1000, evaluate_mode='1by1', hidden=128, homo_boost_thrd=0.8, homo_loss_weight=100, inner=1, k=100, lr=0.01, model='GCN', no_cuda=False, outter_size=4096, prune_thr=0.8, range=1.0, rec_epochs=30, seed=10, selection_method='none', target_class=2, target_loss_weight=1, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=565, vs_ratio=0, weight_decay=0.0005, weight_ood=1, weight_target=1, weight_targetclass=1)\n"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(threshold=10000)\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch_geometric.loader import DataLoader\n",
    "from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated, clu_prune_unrelated_edge\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.utils import subgraph\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='ogbn-arxiv', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Pubmed','Flickr','ogbn-arxiv','Citeseer','Reddit2'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=2)\n",
    "parser.add_argument('--k', type=int, default=100)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=1000, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--rec_epochs', type=int,  default=30, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=400, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=True,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--range', type=float, default=1.0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=565,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"none\",\n",
    "                    choices=['prune', 'isolate', 'none','reconstruct'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.8,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--weight_target', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--weight_ood', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--weight_targetclass', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--outter_size', type=int, default=4096,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=100,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.8,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--selection_method', type=str, default='none',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='1by1',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    from ogb.nodeproppred import PygNodePropPredDataset\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "    \n",
    "if(args.dataset == 'Reddit2'):\n",
    "    num_nodes_to_sample = 20000  # Adjust this based on your needs\n",
    "\n",
    "    # Randomly select a subset of nodes\n",
    "    sampled_nodes = torch.randint(data.num_nodes, (num_nodes_to_sample,), device=device)\n",
    "\n",
    "    # Perform subgraph sampling\n",
    "    edge,_ = subgraph(sampled_nodes, data.edge_index)\n",
    "    data.edge_index = edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_split\n",
    "data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of benign training nodes 33868\n",
      "number of poisoned target nodes 565\n"
     ]
    }
   ],
   "source": [
    "from sklearn_extra import cluster\n",
    "from models.backdoor import Backdoor\n",
    "from models.construct import model_construct\n",
    "import heuristic_selection as hs\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "# select poisoned target node #\n",
    "\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()\n",
    "if(args.use_vs_number):\n",
    "    size = args.vs_number\n",
    "else:\n",
    "    size = int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "# print(\"#Attach Nodes:{}\".format(size))\n",
    "assert size>0, 'The number of selected trigger nodes must be larger than 0!'\n",
    "# here is randomly select poison nodes from unlabeled nodes\n",
    "if(args.selection_method == 'none'):\n",
    "    idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "elif(args.selection_method == 'cluster'):\n",
    "    idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "elif(args.selection_method == 'cluster_degree'):\n",
    "    if(args.dataset == 'Pubmed'):\n",
    "        idx_attach = hs.cluster_degree_selection_seperate_fixed(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    else:\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "# print(\"idx_attach: {}\".format(idx_attach))\n",
    "unlabeled_idx = torch.tensor(list(set(unlabeled_idx.cpu().numpy()) - set(idx_attach.cpu().numpy()))).to(device)\n",
    "print('number of benign training nodes', len(idx_train))\n",
    "print('number of poisoned target nodes', len(idx_attach))\n",
    "\n",
    "# Cora\n",
    "# idx_attach = torch.tensor([1672, 2399, 1785, 2020, 2013, 1652,  208, 1220, 2128,  446])\n",
    "# Flicker\n",
    "idx_attach = torch.tensor([151986, 162467,  44926,  23940,  72501,  13300,  22065, 147153,  79242,\n",
    "        128143,  93465,  63507,  73972,  73044,  56937,  88754, 163842, 119362,\n",
    "         54554,  33519,   8154,  16049, 113091, 156331,  35927, 152431,  38080,\n",
    "        168947, 121840, 101165,  34467, 130677, 127703, 162722,   4875,  95491,\n",
    "        148529,  78072, 126281,  59601,  54103, 107477,  51951,  10138,  10104,\n",
    "         26393,  18876, 151911,  73699, 122090, 149541,   7918,  23625,  54054,\n",
    "        165054,  90478, 130766, 106047,  62056, 152543, 100119,  62343, 119437,\n",
    "        129228, 130514,  12459,  58483,  35161,  27651,  77539,  49360,  43087,\n",
    "         51852,  84642, 162577, 108656,  81292,  49283,  70860,  49404, 141270,\n",
    "         36926, 101724,  14727,  12525,  19995, 162207,  69782, 127237,   8541,\n",
    "         83147, 131520,  97803, 109806, 145193, 118680,  20924, 131955, 106455,\n",
    "        129401,  58891, 116300,  34079,    224,   3172,  88160, 109185, 110668,\n",
    "        103248,  95078, 154019, 168817,   3771,  18207, 154860,  26834,  11175,\n",
    "         63122,  47747,  80305, 102581,  82265, 117214, 132492, 137535, 122891,\n",
    "        164548, 100623,  17308, 153845,  91474, 142746,  85505,  97061, 152602,\n",
    "        116420,  15941, 152529,  19807,  15146, 148629, 162255, 138185,  96655,\n",
    "         71909,  25179,  34595, 147363,  64609,  67502,   8161, 136765, 110630,\n",
    "         11895, 158529,   5181,  90008,  16843,  62749,  33553,  15212,  24861,\n",
    "        160062,  72270,  25230,  76515,  27120, 152018,  95828, 124173,  80523,\n",
    "         28845,  84596,  56397,     63, 164765, 140751,  37912,  47903,  69429,\n",
    "        132506,  66631, 153695,  61748,  18334,  94547,  97448,  58066,   1646,\n",
    "        158386, 153884,  39628, 129569,  86483, 159823, 109138,  52390,  20765,\n",
    "         70732,  56531,  14090, 144949,  45603, 136798, 162265,  66578,  42077,\n",
    "        144225,  23731,  37597,  73034, 115402,  40876, 104645, 123502, 154395,\n",
    "         47564, 112664, 137399,  62430,   6464, 124696,  50606,  34820, 116196,\n",
    "        136597, 145708, 105880, 129765,  57303, 155369, 114620,  95339, 103305,\n",
    "         83091,  68857,  73046,   3183, 149185,  80135, 155473, 164332,  20333,\n",
    "        139918,  45024, 141475,  91437,   2599,  74708, 160785, 168601,   2915,\n",
    "         75096,   5486,  69934,   3244, 108649,  69213,  96039,  74594, 144675,\n",
    "         65112, 157016, 150964,  91340,  17814, 134345,  74006,  51257, 159074,\n",
    "        165012,  78616,  87458,  26847, 138007, 123722,  44990,  70164, 162681,\n",
    "        155937,  70939, 115374, 166985, 112764, 144479,  26546,  53170,  25088,\n",
    "         22346, 157904,  75119,  34364,  98424,  93012, 118674,  89130,  72419,\n",
    "         29984, 117334,  16483,  19802, 113586,  92262, 140284, 157255, 148016,\n",
    "         96550,  35893,  73545,  38199,  60664,   1496, 158258, 135970, 121417,\n",
    "        141379, 164731, 118159,  29576,  75840, 144213,  73692, 104518, 103593,\n",
    "        126236, 108326,  89873, 140364,  97116, 124288, 121316,   5171, 152926,\n",
    "          3575,  68393, 168823, 134560,  97522, 147712, 152664,  14128,   3127,\n",
    "         20516,  85926,  13117, 122575,  59647, 147988,  15855,  38123,  72738,\n",
    "        123088,  28674, 137800, 151853,  40493, 119849, 125141,  28030,  91051,\n",
    "        137817,  32725, 151387,  23191, 123924,  94511, 140409,  84661,  57390,\n",
    "        108036, 146481,  50713,  25635,   8406,  53176,  63767,  10085,  11511,\n",
    "         10917, 125420, 106500, 122872, 131682, 136815,  70797,   9775, 107485,\n",
    "         34616,  38800, 157312, 121295, 144843,  53058,  86092,  78410, 129884,\n",
    "        122432,  34859,  20803,  68352,  62509,  56748,  50567,   7305,  80285,\n",
    "         47840,  27191, 140932,  97673,  36251, 158464,  28447,  90035,  35878,\n",
    "         38402, 150487, 147232,   4757,  65803, 136331,  36847, 129631, 125084,\n",
    "          3774,  93301,  70138, 168983,  39973, 167434,  48993, 134157, 142998,\n",
    "        142631, 136629,  70597,   5918, 164119, 146838, 129454,  56518, 104736,\n",
    "         54268, 127712,  51617,  87842, 131931,  75147,  78176, 124529, 144908,\n",
    "        152599, 109500, 152809, 123358, 113359,  63172,   8716,  47711, 153647,\n",
    "         78756,  85109, 120331,  54454, 157612,  15584,  48592, 128980, 118870,\n",
    "        145262,  94992,  28068,  58458,  41115, 103738,   2955,   6977, 108384,\n",
    "        168777,  70271,  89741, 131143, 109832, 126654,  92179, 100438,  58805,\n",
    "        103885, 147804, 113577,  21148,  75731, 100325,  68288,  79565,  28340,\n",
    "          8655,   4924, 156493, 148494,  32973,  44218, 151410,  65225,  69072,\n",
    "         49753,  72095,  15442, 113605,  69861, 108791, 123205, 121298, 162781,\n",
    "         82963, 159931,  34136,  10300,  39329, 157392, 107938,    494,  22166,\n",
    "        109920,  17234,  79404,  53596, 134072,  92397,   2688, 121386, 152968,\n",
    "         10262,  85337, 112591,   8951,  59919,  34034, 125602,  44639,  97368,\n",
    "          4367,  40594,  68981, 168198,  69316,  87326,  68237,  35037, 126689,\n",
    "        154968, 163394, 152383, 103946, 126176,  68477, 112778, 160670, 142487,\n",
    "        147511, 138188,  27413, 112906,  49367,  76346,  61945])\n",
    "\n",
    "idx_attach = idx_attach.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## trigger generator ##\n",
    "model = Backdoor(args,device)\n",
    "model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = 'GCN'\n",
    "total_overall_asr = 0\n",
    "total_overall_ca = 0\n",
    "args.test_model = test_model\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=1)\n",
    "overall_asr = 0\n",
    "overall_ca = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load poisoned graph #\n",
    "# poison_x = torch.load('poison_x.pt')\n",
    "# poison_edge_index = torch.load('poison_edge_index.pt')\n",
    "# poison_edge_weights = torch.load('poison_edge_weights.pt')\n",
    "# poison_labels = torch.load('poison_labels.pt')\n",
    "poison_x = torch.load('poison_x.pt')\n",
    "poison_edge_index = torch.load('poison_edge_index.pt')\n",
    "poison_edge_weights = torch.load('poison_edge_weights.pt')\n",
    "poison_labels = torch.load('poison_labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(cuda=True, dataset='ogbn-arxiv', debug=True, defense_mode='none', device_id=2, dis_weight=1, dropout=0.5, epochs=1000, evaluate_mode='1by1', hidden=128, homo_boost_thrd=0.8, homo_loss_weight=100, inner=1, k=100, lr=0.01, model='GCN', no_cuda=False, outter_size=4096, prune_thr=0.8, range=1.0, rec_epochs=30, seed=10, selection_method='none', target_class=2, target_loss_weight=1, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=565, vs_ratio=0, weight_decay=0.0005, weight_ood=1, weight_target=1, weight_targetclass=1)\n"
     ]
    }
   ],
   "source": [
    "print(args)\n",
    "\n",
    "mask = data.y[idx_attach] != args.target_class\n",
    "mask = mask.to(device)\n",
    "\n",
    "## only attack those has groud truth labels != target_class ##\n",
    "idx_attach = idx_attach[(data.y[idx_attach] != args.target_class).nonzero().flatten()]\n",
    "\n",
    "bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "# test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "known_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "predictions = []\n",
    "# edge weight for clean edge_index, may use later #\n",
    "edge_weight = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(544, device='cuda:2')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24, 24,  8, 30, 10, 28, 28, 37,  4, 24, 30, 24,  5, 24, 24,  5, 22,  8,\n",
      "         4, 24, 28, 36, 28, 16, 34, 10, 16, 24, 37, 24, 24,  7, 10, 38, 28,  6,\n",
      "        16, 10,  8, 24, 28, 23, 10, 16, 28, 28, 30, 22,  4,  4, 24, 13, 28, 28,\n",
      "         1, 28, 30, 16, 16, 16, 34, 14, 27, 16,  6, 16, 28, 28, 28,  4,  3, 24,\n",
      "        23, 24, 34, 34, 16,  1, 39, 30, 16, 38, 30, 34, 24, 30, 34, 16, 36, 23,\n",
      "        27, 37,  9,  4, 24, 19, 28, 34, 16, 28, 10, 15, 24, 24,  9, 28, 16, 19,\n",
      "         4, 16,  9, 24, 23, 23, 31, 22,  8, 33, 28, 24, 24, 24,  8,  9, 17, 16,\n",
      "        13, 28, 13, 28, 16, 28, 16, 10, 30, 30, 16, 24, 28,  6, 10, 28, 30, 28,\n",
      "        16, 16, 39, 28,  9, 34, 24, 24, 23,  0, 25, 16, 16, 30, 36, 28,  4, 16,\n",
      "         3, 24,  9,  9,  8, 24,  5, 13, 19, 24, 30, 31, 24, 20,  5, 19, 17, 16,\n",
      "        34, 19, 21, 19, 28, 10,  8, 16, 34,  8, 37, 24,  4, 37, 28, 16, 28,  4,\n",
      "        16, 26, 16, 30, 24, 19,  8, 24, 36,  4, 24,  5, 31, 19, 28, 16, 23, 30,\n",
      "         9, 24, 33, 23, 30, 34, 34, 10, 26, 16, 10, 31, 28, 24,  5, 38, 34, 19,\n",
      "        16, 10, 37, 18, 16, 30, 27, 26, 34, 28, 36, 31, 28, 31, 13, 16,  4, 10,\n",
      "         0, 16, 33, 22, 27, 23,  4, 16,  8,  5, 28, 16, 28,  4, 28, 27, 28, 30,\n",
      "        19, 28, 28, 28, 10, 31, 36, 13, 34, 30, 28, 24,  5, 24, 28, 30,  5, 30,\n",
      "         9, 19,  5, 16, 34, 24,  3, 16, 16, 26, 24, 30, 34, 16, 16, 28,  4, 28,\n",
      "         6, 28, 16, 31, 16, 24, 22, 28, 24,  6, 16, 30, 34,  3, 16, 28, 26, 13,\n",
      "        30, 28, 28,  0, 27,  5, 28, 28, 31, 24, 27, 39, 23, 13, 10, 27, 34, 24,\n",
      "        36, 10, 25, 28, 30, 34, 20, 28, 28, 26,  3, 16, 19, 34, 28,  4, 16, 36,\n",
      "        28, 37, 16, 28, 30, 16, 30, 23, 10,  4, 30, 34, 24,  7, 16, 30, 33, 10,\n",
      "        34, 31, 16,  8,  3, 22, 10, 28,  5, 16, 39, 16, 24, 20, 27, 19, 25, 28,\n",
      "         4, 24, 30, 34, 24, 28, 28,  8, 30, 24, 16, 16, 38, 39, 16, 25, 28, 30,\n",
      "        16, 30, 19, 23, 10, 16, 24, 16, 36, 16,  8,  5, 10, 34,  5, 28, 24, 16,\n",
      "        27, 27, 39, 16, 24, 28, 28, 16, 28, 24, 26, 24, 10, 16, 28, 34, 31, 16,\n",
      "        16, 22, 24, 31, 16,  3, 28, 16, 16, 20, 24, 36, 26,  5,  8,  8,  4, 19,\n",
      "        16, 24,  8, 24, 20, 27,  4, 28,  8, 16, 31, 22, 39,  3, 28,  3, 16, 34,\n",
      "        28,  4, 30, 31, 19, 28,  3, 13, 16, 28, 24, 24, 16,  4,  4, 22, 24,  5,\n",
      "        27, 16, 36, 36, 34, 34, 39, 16, 34, 24, 26,  8, 25, 24, 31, 16, 34, 27,\n",
      "        28, 15, 31, 39, 16, 19,  8, 37, 30,  8, 19, 10, 34, 34, 30, 16,  0, 37,\n",
      "         6, 24, 28, 10], device='cuda:2')\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "print(data.y[idx_attach])\n",
    "# idx_attach is selected target node #\n",
    "print(poison_labels[idx_attach])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on poisoned target nodes: 0.9963\n"
     ]
    }
   ],
   "source": [
    "# train a backdoored model on poisoned graph # \n",
    "test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "test_model.fit(poison_x,poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "test_model.eval()\n",
    "clean_acc = test_model.test(poison_x,poison_edge_index, poison_edge_weights,poison_labels,idx_attach)\n",
    "output, x = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "ori_predict = torch.exp(output[known_nodes])\n",
    "print(\"accuracy on poisoned target nodes: {:.4f}\".format(clean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR: 0.9926\n",
      "Flip ASR: 0.9924/16482 nodes\n",
      "CA: 0.6713\n"
     ]
    }
   ],
   "source": [
    "# test backdoored model for comparison #\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "\n",
    "output, x = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "asr = train_attach_rate\n",
    "flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "print(\"CA: {:.4f}\".format(ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge drop #\n",
    "def sample_noise_all(edge_index, edge_weight,device):\n",
    "    noisy_edge_index = edge_index.clone().detach()\n",
    "    if(edge_weight == None):\n",
    "        noisy_edge_weight = torch.ones([noisy_edge_index.shape[1],]).to(device)\n",
    "    else:\n",
    "        noisy_edge_weight = edge_weight.clone().detach()\n",
    "    # # rand_noise_data = copy.deepcopy(data)\n",
    "    # rand_noise_data.edge_weight = torch.ones([rand_noise_data.edge_index.shape[1],]).to(device)\n",
    "    m = Bernoulli(torch.tensor([0.5]).to(device))\n",
    "    mask = m.sample(noisy_edge_weight.shape).squeeze(-1).int()\n",
    "    # print('mask',mask)\n",
    "    rand_inputs = torch.randint_like(noisy_edge_weight, low=0, high=2).squeeze().int().to(device)\n",
    "    # print(rand_noise_data.edge_weight.shape,mask.shape)\n",
    "    noisy_edge_weight = noisy_edge_weight * mask #+ rand_inputs * (1-mask)\n",
    "        \n",
    "    if(noisy_edge_weight!=None):\n",
    "        noisy_edge_index = noisy_edge_index[:,noisy_edge_weight.nonzero().flatten().long()]\n",
    "        noisy_edge_weight = torch.ones([noisy_edge_index.shape[1],]).to(device)\n",
    "    return noisy_edge_index, noisy_edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test robustness #\n",
    "for i in range(20):\n",
    "            test_model.eval()\n",
    "            noisy_poison_edge_index, noisy_poison_edge_weights = sample_noise_all(poison_edge_index, poison_edge_weights, device)\n",
    "            output, x = test_model(poison_x,noisy_poison_edge_index,noisy_poison_edge_weights)\n",
    "            train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "            train_clean_rate = (output.argmax(dim=1)[idx_train]==data.y[idx_train]).float().mean()\n",
    "            predictions.append(torch.exp(output[known_nodes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.argmax(predictions[8][23868])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_of_less_robust tensor([ 6730, 22430, 33868, 33869, 33870, 33871, 33872, 33873, 33874, 33875,\n",
      "        33876, 33877, 33878, 33879, 33880, 33881, 33882, 33883, 33884, 33885,\n",
      "        33886, 33887, 33888, 33889, 33890, 33891, 33892, 33893, 33894, 33895,\n",
      "        33896, 33897, 33898, 33899, 33900, 33901, 33902, 33903, 33904, 33905,\n",
      "        33906, 33907, 33908, 33909, 33910, 33911, 33912, 33913, 33914, 33915,\n",
      "        33916, 33917, 33918, 33919, 33920, 33921, 33922, 33923, 33924, 33925,\n",
      "        33926, 33927, 33928, 33929, 33930, 33931, 33932, 33933, 33934, 33935,\n",
      "        33936, 33937, 33938, 33939, 33940, 33941, 33942, 33943, 33944, 33945,\n",
      "        33946, 33947, 33948, 33949, 33950, 33951, 33952, 33953, 33954, 33955,\n",
      "        33956, 33957, 33958, 33959, 33960, 33961, 33962, 33963, 33964, 33965,\n",
      "        33966, 33967, 33968, 33969, 33970, 33971, 33972, 33973, 33974, 33975,\n",
      "        33976, 33977, 33978, 33979, 33980, 33981, 33982, 33983, 33984, 33985,\n",
      "        33986, 33987, 33988, 33989, 33990, 33991, 33992, 33993, 33994, 33995,\n",
      "        33996, 33997, 33998, 33999, 34000, 34001, 34002, 34003, 34004, 34005,\n",
      "        34006, 34007, 34008, 34009, 34010, 34011, 34012, 34013, 34014, 34015,\n",
      "        34016, 34017, 34018, 34019, 34020, 34021, 34022, 34023, 34024, 34025,\n",
      "        34026, 34027, 34028, 34029, 34030, 34031, 34032, 34033, 34034, 34035,\n",
      "        34036, 34037, 34038, 34039, 34040, 34041, 34042, 34043, 34044, 34045,\n",
      "        34046, 34047, 34048, 34049, 34050, 34051, 34052, 34053, 34054, 34055,\n",
      "        34056, 34057, 34058, 34059, 34060, 34061, 34062, 34063, 34064, 34065,\n",
      "        34067, 34068, 34069, 34070, 34071, 34072, 34073, 34074, 34075, 34076,\n",
      "        34077, 34078, 34079, 34080, 34081, 34082, 34083, 34084, 34086, 34087,\n",
      "        34088, 34089, 34090, 34091, 34092, 34093, 34094, 34095, 34096, 34097,\n",
      "        34098, 34099, 34100, 34101, 34102, 34103, 34104, 34105, 34106, 34107,\n",
      "        34108, 34109, 34110, 34111, 34112, 34113, 34114, 34115, 34116, 34117,\n",
      "        34118, 34119, 34120, 34121, 34122, 34123, 34124, 34125, 34126, 34127,\n",
      "        34128, 34129, 34130, 34131, 34132, 34133, 34134, 34135, 34136, 34137,\n",
      "        34138, 34139, 34140, 34141, 34142, 34143, 34144, 34145, 34146, 34147,\n",
      "        34148, 34149, 34150, 34151, 34152, 34153, 34154, 34156, 34157, 34158,\n",
      "        34159, 34160, 34161, 34162, 34163, 34164, 34165, 34166, 34167, 34168,\n",
      "        34169, 34170, 34171, 34172, 34173, 34174, 34175, 34176, 34177, 34178,\n",
      "        34179, 34180, 34181, 34182, 34183, 34184, 34185, 34186, 34187, 34188,\n",
      "        34189, 34190, 34191, 34192, 34193, 34194, 34195, 34196, 34197, 34198,\n",
      "        34199, 34200, 34201, 34202, 34203, 34204, 34205, 34206, 34207, 34208,\n",
      "        34209, 34210, 34211, 34212, 34213, 34214, 34215, 34216, 34217, 34218,\n",
      "        34219, 34220, 34221, 34222, 34223, 34224, 34225, 34226, 34227, 34228,\n",
      "        34229, 34230, 34231, 34232, 34233, 34234, 34235, 34237, 34238, 34239,\n",
      "        34240, 34241, 34242, 34243, 34244, 34245, 34246, 34247, 34248, 34249,\n",
      "        34250, 34251, 34252, 34253, 34254, 34255, 34256, 34257, 34258, 34259,\n",
      "        34260, 34261, 34262, 34263, 34264, 34265, 34266, 34267, 34268, 34269,\n",
      "        34270, 34271, 34272, 34273, 34274, 34275, 34276, 34277, 34278, 34279,\n",
      "        34280, 34281, 34282, 34283, 34284, 34285, 34286, 34287, 34288, 34289,\n",
      "        34290, 34291, 34292, 34293, 34295, 34296, 34297, 34298, 34299, 34300,\n",
      "        34301, 34302, 34303, 34304, 34305, 34306, 34308, 34309, 34310, 34311,\n",
      "        34312, 34313, 34314, 34315, 34316, 34317, 34319, 34320, 34321, 34322,\n",
      "        34323, 34324, 34325, 34326, 34327, 34328, 34329, 34330, 34331, 34332,\n",
      "        34333, 34334, 34335, 34336, 34337, 34338, 34339, 34340, 34341, 34342,\n",
      "        34343, 34344, 34345, 34346, 34347, 34348, 34349, 34350, 34351, 34352,\n",
      "        34353, 34354, 34355, 34356, 34357, 34358, 34359, 34360, 34361, 34362,\n",
      "        34363, 34364, 34365, 34366, 34367, 34368, 34369, 34370, 34371, 34372,\n",
      "        34373, 34374, 34375, 34376, 34377, 34378, 34379, 34380, 34381, 34382,\n",
      "        34383, 34384, 34385, 34386, 34387, 34388, 34389, 34390, 34391, 34392,\n",
      "        34393, 34394, 34395, 34396, 34397, 34398, 34399, 34400, 34401, 34402,\n",
      "        34403, 34404, 34405, 34406, 34407, 34408, 34409, 34410, 34411, 34307,\n",
      "        34318, 34155, 14587, 34236], device='cuda:2')\n",
      "count 541\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-8\n",
    "deviations = []\n",
    "for sub_pred in predictions:\n",
    "    sub_pred += epsilon\n",
    "    deviation = F.kl_div(sub_pred.log(), ori_predict, reduce=False)\n",
    "    deviations.append(deviation)\n",
    "\n",
    "summed_deviations = torch.zeros_like(deviations[0]).to(deviations[0].device)\n",
    "for deviation in deviations:\n",
    "    ##### summed deviations for each node #####\n",
    "    summed_deviations += deviation\n",
    "\n",
    "\n",
    "##### get the index for nodes with less robustness #####\n",
    "    \n",
    "##### args.vs_number is unknown #####\n",
    "index_of_less_robust = torch.sort(torch.mean(summed_deviations,dim=-1),descending=True)[1][:mask.sum()]\n",
    "print('index_of_less_robust',index_of_less_robust)\n",
    "\n",
    "##### count how many poisoned target nodes are selected in less robustness nodes #####\n",
    "count = 0\n",
    "dd = []\n",
    "for idx in index_of_less_robust:\n",
    "    if idx >= len(known_nodes)-args.vs_number:\n",
    "        count += 1\n",
    "        dd.append(idx)\n",
    "print('count',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
       "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf, 6.6134,\n",
       "        6.4871, 2.9039, 2.8398, 2.7770], device='cuda:2',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sort(torch.mean(summed_deviations,dim=-1),descending=True)[0][:mask.sum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(544, device='cuda:2')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ori_predict))\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10, device='cuda:2')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(predictions[4][543])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "supplied range of [0.0, inf] is not finite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# plt.figure(figsize=(40, 24))\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m##### benign nodes #####\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_excluding_last_40\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbenign nodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m##### poisoned success nodes #####\u001b[39;00m\n\u001b[1;32m     40\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(a_last_40, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoisoned target nodes\u001b[39m\u001b[38;5;124m'\u001b[39m,density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/spurious/lib/python3.8/site-packages/matplotlib/pyplot.py:2602\u001b[0m, in \u001b[0;36mhist\u001b[0;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001b[0m\n\u001b[1;32m   2596\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mhist)\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhist\u001b[39m(\n\u001b[1;32m   2598\u001b[0m         x, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mrange\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2599\u001b[0m         cumulative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, bottom\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, histtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m, align\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmid\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2600\u001b[0m         orientation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m, rwidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2601\u001b[0m         label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stacked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdensity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcumulative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcumulative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbottom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhisttype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhisttype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spurious/lib/python3.8/site-packages/matplotlib/__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1414\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1415\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1416\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/anaconda3/envs/spurious/lib/python3.8/site-packages/matplotlib/axes/_axes.py:6635\u001b[0m, in \u001b[0;36mAxes.hist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   6631\u001b[0m \u001b[38;5;66;03m# Loop through datasets\u001b[39;00m\n\u001b[1;32m   6632\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nx):\n\u001b[1;32m   6633\u001b[0m     \u001b[38;5;66;03m# this will automatically overwrite bins,\u001b[39;00m\n\u001b[1;32m   6634\u001b[0m     \u001b[38;5;66;03m# so that each histogram uses the same bins\u001b[39;00m\n\u001b[0;32m-> 6635\u001b[0m     m, bins \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhist_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6636\u001b[0m     tops\u001b[38;5;241m.\u001b[39mappend(m)\n\u001b[1;32m   6637\u001b[0m tops \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tops, \u001b[38;5;28mfloat\u001b[39m)  \u001b[38;5;66;03m# causes problems later if it's an int\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/spurious/lib/python3.8/site-packages/numpy/lib/histograms.py:780\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, density, weights)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m \n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    778\u001b[0m a, weights \u001b[38;5;241m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[0;32m--> 780\u001b[0m bin_edges, uniform_bins \u001b[38;5;241m=\u001b[39m \u001b[43m_get_bin_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/spurious/lib/python3.8/site-packages/numpy/lib/histograms.py:426\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_equal_bins \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m \u001b[43m_get_outer_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(bins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    429\u001b[0m     bin_edges \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(bins)\n",
      "File \u001b[0;32m~/anaconda3/envs/spurious/lib/python3.8/site-packages/numpy/lib/histograms.py:315\u001b[0m, in \u001b[0;36m_get_outer_edges\u001b[0;34m(a, range)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax must be larger than min in range parameter.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misfinite(first_edge) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(last_edge)):\n\u001b[0;32m--> 315\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    316\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupplied range of [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] is not finite\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(first_edge, last_edge))\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m a\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# handle empty arrays. Can't determine range, so use 0-1.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: supplied range of [0.0, inf] is not finite"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# visualize robustness distribution #\n",
    "a=torch.mean(summed_deviations,dim=-1)\n",
    "a_excluding_last_40 = a[:-args.vs_number].detach().cpu()\n",
    "a_last=a[-mask.sum():].detach().cpu()\n",
    "\n",
    "\n",
    "a_last_40 = a[-args.vs_number:].detach().cpu()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "indices_to_remove = [i for i, node in enumerate(known_nodes[-args.vs_number:]) if data.y[node] == args.target_class]\n",
    "\n",
    "indices_to_remove = []\n",
    "\n",
    "a_last_list = list(a_last)\n",
    "\n",
    "indices_to_check = range(len(known_nodes[-args.vs_number:]))\n",
    "\n",
    "# indices_to_remove = []\n",
    "\n",
    "# ##### find those nodes that are already have target class or poisoned failed #####\n",
    "# for i in indices_to_check:\n",
    "#     condition1 = data.y[known_nodes[-args.vs_number:][i]] == args.target_class\n",
    "#     condition2 = output.argmax(dim=1)[idx_attach[i]] != args.target_class\n",
    "    \n",
    "#     if condition1 or condition2:\n",
    "#         indices_to_remove.append(i)\n",
    "\n",
    "# for index in sorted(indices_to_remove, reverse=True):\n",
    "#     del a_last_list[index]\n",
    "\n",
    "\n",
    "a_last_40 = a_last_list\n",
    "plt.figure(figsize=(10, 6))\n",
    "# plt.figure(figsize=(40, 24))\n",
    "##### benign nodes #####\n",
    "plt.hist(a_excluding_last_40, bins=20, alpha=0.5, label='benign nodes', density=True)\n",
    "##### poisoned success nodes #####\n",
    "plt.hist(a_last_40, bins=20, alpha=0.5, label='poisoned target nodes',density=True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of robustness for each node')\n",
    "plt.savefig('a.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "       device='cuda:2')\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "       device='cuda:2')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# retrain a model #\u001b[39;00m\n\u001b[1;32m     14\u001b[0m test_model \u001b[38;5;241m=\u001b[39m model_construct(args,args\u001b[38;5;241m.\u001b[39mtest_model,data,device)\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[0;32m---> 15\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoison_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpoison_edge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoison_edge_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoison_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbkd_tn_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m## test model on purified graph (poisoned target node) ##\u001b[39;00m\n\u001b[1;32m     18\u001b[0m clean_acc \u001b[38;5;241m=\u001b[39m test_model\u001b[38;5;241m.\u001b[39mtest(poison_x,poison_edge_index, poison_edge_weights,poison_labels,idx_attach)\n",
      "File \u001b[0;32m/data/home/zbz5349/defense_backdoor/pre/models/GCN.py:97\u001b[0m, in \u001b[0;36mGCN.fit\u001b[0;34m(self, features, edge_index, edge_weight, labels, idx_train, idx_val, train_iters, verbose)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_without_val(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels, idx_train, train_iters, verbose)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_with_val\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/home/zbz5349/defense_backdoor/pre/models/GCN.py:138\u001b[0m, in \u001b[0;36mGCN._train_with_val\u001b[0;34m(self, labels, idx_train, idx_val, train_iters, verbose)\u001b[0m\n\u001b[1;32m    133\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 138\u001b[0m output, x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output[idx_val], labels[idx_val])\n\u001b[1;32m    140\u001b[0m acc_val \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39maccuracy(output[idx_val], labels[idx_val])\n",
      "File \u001b[0;32m/data/home/zbz5349/defense_backdoor/pre/models/GCN.py:49\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m     47\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[0;32m---> 49\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ln:\n\u001b[1;32m     51\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlns[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n",
      "File \u001b[0;32m~/anaconda3/envs/spurious/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/spurious/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:172\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    170\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/spurious/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:58\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, dtype)\u001b[0m\n\u001b[1;32m     54\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m     55\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 58\u001b[0m     edge_index, tmp_edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m tmp_edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m tmp_edge_weight\n",
      "File \u001b[0;32m~/anaconda3/envs/spurious/lib/python3.8/site-packages/torch_geometric/utils/loop.py:228\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    224\u001b[0m     loop_attr[edge_index[\u001b[38;5;241m0\u001b[39m][inv_mask]] \u001b[38;5;241m=\u001b[39m edge_attr[inv_mask]\n\u001b[1;32m    226\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_attr[mask], loop_attr], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 228\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m, loop_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m edge_index, edge_attr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## retrain a model on purified graph ##\n",
    "\n",
    "# labels for target poisoned node before purified #\n",
    "print(poison_labels[idx_attach])\n",
    "\n",
    "# purifying #\n",
    "for idx in index_of_less_robust:\n",
    "    poison_labels[bkd_tn_nodes[idx]]=data.y[bkd_tn_nodes[idx]]\n",
    "\n",
    "# labels for target poisoned node after purified #\n",
    "print(poison_labels[idx_attach])\n",
    "\n",
    "# retrain a model #\n",
    "test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "test_model.fit(poison_x,poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "## test model on purified graph (poisoned target node) ##\n",
    "clean_acc = test_model.test(poison_x,poison_edge_index, poison_edge_weights,poison_labels,idx_attach)\n",
    "\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR: 0.9915\n",
      "Flip ASR: 0.9875/4971 nodes\n",
      "CA: 0.4381\n"
     ]
    }
   ],
   "source": [
    "# add a trigger detector #\n",
    "# assumption: backdoor attack's success is based on trigger pattern #\n",
    "# 1. outlier, trigger different to each other #\n",
    "# 2. in distribution, trigger similar to each other #\n",
    "# # in case, model trained on clean graph learn attack pattern #\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "\n",
    "output, x = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "asr = train_attach_rate\n",
    "flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "print(\"CA: {:.4f}\".format(ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spurious",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
