{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1141857/3417948181.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/zbz5349/anaconda3/envs/spurious/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(cuda=True, dataset='ogbn-arxiv', debug=True, defense_mode='none', device_id=2, dis_weight=1, dropout=0.5, epochs=1000, evaluate_mode='1by1', hidden=128, homo_boost_thrd=0.8, homo_loss_weight=100, inner=1, k=100, lr=0.01, model='GCN', no_cuda=False, outter_size=4096, prune_thr=0.8, range=1.0, rec_epochs=30, seed=10, selection_method='none', target_class=2, target_loss_weight=1, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=565, vs_ratio=0, weight_decay=0.0005, weight_ood=1, weight_target=1, weight_targetclass=1)\n"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(threshold=10000)\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch_geometric.loader import DataLoader\n",
    "from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated, clu_prune_unrelated_edge\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.utils import subgraph\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='ogbn-arxiv', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Pubmed','Flickr','ogbn-arxiv','Citeseer','Reddit2'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=2)\n",
    "parser.add_argument('--k', type=int, default=100)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=1000, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--rec_epochs', type=int,  default=30, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=400, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=True,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--range', type=float, default=1.0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=565,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"none\",\n",
    "                    choices=['prune', 'isolate', 'none','reconstruct'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.8,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--weight_target', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--weight_ood', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--weight_targetclass', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--outter_size', type=int, default=4096,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=100,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.8,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--selection_method', type=str, default='none',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='1by1',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    from ogb.nodeproppred import PygNodePropPredDataset\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "    \n",
    "if(args.dataset == 'Reddit2'):\n",
    "    num_nodes_to_sample = 20000  # Adjust this based on your needs\n",
    "\n",
    "    # Randomly select a subset of nodes\n",
    "    sampled_nodes = torch.randint(data.num_nodes, (num_nodes_to_sample,), device=device)\n",
    "\n",
    "    # Perform subgraph sampling\n",
    "    edge,_ = subgraph(sampled_nodes, data.edge_index)\n",
    "    data.edge_index = edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_split\n",
    "data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of benign training nodes 33868\n",
      "number of poisoned target nodes 565\n"
     ]
    }
   ],
   "source": [
    "from sklearn_extra import cluster\n",
    "from models.backdoor import Backdoor\n",
    "from models.construct import model_construct\n",
    "import heuristic_selection as hs\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "# select poisoned target node #\n",
    "\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()\n",
    "if(args.use_vs_number):\n",
    "    size = args.vs_number\n",
    "else:\n",
    "    size = int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "# print(\"#Attach Nodes:{}\".format(size))\n",
    "assert size>0, 'The number of selected trigger nodes must be larger than 0!'\n",
    "# here is randomly select poison nodes from unlabeled nodes\n",
    "if(args.selection_method == 'none'):\n",
    "    idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "elif(args.selection_method == 'cluster'):\n",
    "    idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "elif(args.selection_method == 'cluster_degree'):\n",
    "    if(args.dataset == 'Pubmed'):\n",
    "        idx_attach = hs.cluster_degree_selection_seperate_fixed(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    else:\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "# print(\"idx_attach: {}\".format(idx_attach))\n",
    "unlabeled_idx = torch.tensor(list(set(unlabeled_idx.cpu().numpy()) - set(idx_attach.cpu().numpy()))).to(device)\n",
    "print('number of benign training nodes', len(idx_train))\n",
    "print('number of poisoned target nodes', len(idx_attach))\n",
    "\n",
    "# Cora\n",
    "# idx_attach = torch.tensor([1672, 2399, 1785, 2020, 2013, 1652,  208, 1220, 2128,  446])\n",
    "# Flicker\n",
    "idx_attach = torch.tensor([151986, 162467,  44926,  23940,  72501,  13300,  22065, 147153,  79242,\n",
    "        128143,  93465,  63507,  73972,  73044,  56937,  88754, 163842, 119362,\n",
    "         54554,  33519,   8154,  16049, 113091, 156331,  35927, 152431,  38080,\n",
    "        168947, 121840, 101165,  34467, 130677, 127703, 162722,   4875,  95491,\n",
    "        148529,  78072, 126281,  59601,  54103, 107477,  51951,  10138,  10104,\n",
    "         26393,  18876, 151911,  73699, 122090, 149541,   7918,  23625,  54054,\n",
    "        165054,  90478, 130766, 106047,  62056, 152543, 100119,  62343, 119437,\n",
    "        129228, 130514,  12459,  58483,  35161,  27651,  77539,  49360,  43087,\n",
    "         51852,  84642, 162577, 108656,  81292,  49283,  70860,  49404, 141270,\n",
    "         36926, 101724,  14727,  12525,  19995, 162207,  69782, 127237,   8541,\n",
    "         83147, 131520,  97803, 109806, 145193, 118680,  20924, 131955, 106455,\n",
    "        129401,  58891, 116300,  34079,    224,   3172,  88160, 109185, 110668,\n",
    "        103248,  95078, 154019, 168817,   3771,  18207, 154860,  26834,  11175,\n",
    "         63122,  47747,  80305, 102581,  82265, 117214, 132492, 137535, 122891,\n",
    "        164548, 100623,  17308, 153845,  91474, 142746,  85505,  97061, 152602,\n",
    "        116420,  15941, 152529,  19807,  15146, 148629, 162255, 138185,  96655,\n",
    "         71909,  25179,  34595, 147363,  64609,  67502,   8161, 136765, 110630,\n",
    "         11895, 158529,   5181,  90008,  16843,  62749,  33553,  15212,  24861,\n",
    "        160062,  72270,  25230,  76515,  27120, 152018,  95828, 124173,  80523,\n",
    "         28845,  84596,  56397,     63, 164765, 140751,  37912,  47903,  69429,\n",
    "        132506,  66631, 153695,  61748,  18334,  94547,  97448,  58066,   1646,\n",
    "        158386, 153884,  39628, 129569,  86483, 159823, 109138,  52390,  20765,\n",
    "         70732,  56531,  14090, 144949,  45603, 136798, 162265,  66578,  42077,\n",
    "        144225,  23731,  37597,  73034, 115402,  40876, 104645, 123502, 154395,\n",
    "         47564, 112664, 137399,  62430,   6464, 124696,  50606,  34820, 116196,\n",
    "        136597, 145708, 105880, 129765,  57303, 155369, 114620,  95339, 103305,\n",
    "         83091,  68857,  73046,   3183, 149185,  80135, 155473, 164332,  20333,\n",
    "        139918,  45024, 141475,  91437,   2599,  74708, 160785, 168601,   2915,\n",
    "         75096,   5486,  69934,   3244, 108649,  69213,  96039,  74594, 144675,\n",
    "         65112, 157016, 150964,  91340,  17814, 134345,  74006,  51257, 159074,\n",
    "        165012,  78616,  87458,  26847, 138007, 123722,  44990,  70164, 162681,\n",
    "        155937,  70939, 115374, 166985, 112764, 144479,  26546,  53170,  25088,\n",
    "         22346, 157904,  75119,  34364,  98424,  93012, 118674,  89130,  72419,\n",
    "         29984, 117334,  16483,  19802, 113586,  92262, 140284, 157255, 148016,\n",
    "         96550,  35893,  73545,  38199,  60664,   1496, 158258, 135970, 121417,\n",
    "        141379, 164731, 118159,  29576,  75840, 144213,  73692, 104518, 103593,\n",
    "        126236, 108326,  89873, 140364,  97116, 124288, 121316,   5171, 152926,\n",
    "          3575,  68393, 168823, 134560,  97522, 147712, 152664,  14128,   3127,\n",
    "         20516,  85926,  13117, 122575,  59647, 147988,  15855,  38123,  72738,\n",
    "        123088,  28674, 137800, 151853,  40493, 119849, 125141,  28030,  91051,\n",
    "        137817,  32725, 151387,  23191, 123924,  94511, 140409,  84661,  57390,\n",
    "        108036, 146481,  50713,  25635,   8406,  53176,  63767,  10085,  11511,\n",
    "         10917, 125420, 106500, 122872, 131682, 136815,  70797,   9775, 107485,\n",
    "         34616,  38800, 157312, 121295, 144843,  53058,  86092,  78410, 129884,\n",
    "        122432,  34859,  20803,  68352,  62509,  56748,  50567,   7305,  80285,\n",
    "         47840,  27191, 140932,  97673,  36251, 158464,  28447,  90035,  35878,\n",
    "         38402, 150487, 147232,   4757,  65803, 136331,  36847, 129631, 125084,\n",
    "          3774,  93301,  70138, 168983,  39973, 167434,  48993, 134157, 142998,\n",
    "        142631, 136629,  70597,   5918, 164119, 146838, 129454,  56518, 104736,\n",
    "         54268, 127712,  51617,  87842, 131931,  75147,  78176, 124529, 144908,\n",
    "        152599, 109500, 152809, 123358, 113359,  63172,   8716,  47711, 153647,\n",
    "         78756,  85109, 120331,  54454, 157612,  15584,  48592, 128980, 118870,\n",
    "        145262,  94992,  28068,  58458,  41115, 103738,   2955,   6977, 108384,\n",
    "        168777,  70271,  89741, 131143, 109832, 126654,  92179, 100438,  58805,\n",
    "        103885, 147804, 113577,  21148,  75731, 100325,  68288,  79565,  28340,\n",
    "          8655,   4924, 156493, 148494,  32973,  44218, 151410,  65225,  69072,\n",
    "         49753,  72095,  15442, 113605,  69861, 108791, 123205, 121298, 162781,\n",
    "         82963, 159931,  34136,  10300,  39329, 157392, 107938,    494,  22166,\n",
    "        109920,  17234,  79404,  53596, 134072,  92397,   2688, 121386, 152968,\n",
    "         10262,  85337, 112591,   8951,  59919,  34034, 125602,  44639,  97368,\n",
    "          4367,  40594,  68981, 168198,  69316,  87326,  68237,  35037, 126689,\n",
    "        154968, 163394, 152383, 103946, 126176,  68477, 112778, 160670, 142487,\n",
    "        147511, 138188,  27413, 112906,  49367,  76346,  61945])\n",
    "\n",
    "idx_attach = idx_attach.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## trigger generator ##\n",
    "model = Backdoor(args,device)\n",
    "model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = 'GCN'\n",
    "total_overall_asr = 0\n",
    "total_overall_ca = 0\n",
    "args.test_model = test_model\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=1)\n",
    "overall_asr = 0\n",
    "overall_ca = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load poisoned graph #\n",
    "# poison_x = torch.load('poison_x.pt')\n",
    "# poison_edge_index = torch.load('poison_edge_index.pt')\n",
    "# poison_edge_weights = torch.load('poison_edge_weights.pt')\n",
    "# poison_labels = torch.load('poison_labels.pt')\n",
    "poison_x = torch.load('poison_x_arxiv.pt')\n",
    "poison_edge_index = torch.load('poison_edge_index_arxiv.pt')\n",
    "poison_edge_weights = torch.load('poison_edge_weights_arxiv.pt')\n",
    "poison_labels = torch.load('poison_labels_arxiv.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(cuda=True, dataset='ogbn-arxiv', debug=True, defense_mode='none', device_id=2, dis_weight=1, dropout=0.5, epochs=1000, evaluate_mode='1by1', hidden=128, homo_boost_thrd=0.8, homo_loss_weight=100, inner=1, k=100, lr=0.01, model='GCN', no_cuda=False, outter_size=4096, prune_thr=0.8, range=1.0, rec_epochs=30, seed=10, selection_method='none', target_class=2, target_loss_weight=1, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=565, vs_ratio=0, weight_decay=0.0005, weight_ood=1, weight_target=1, weight_targetclass=1)\n"
     ]
    }
   ],
   "source": [
    "print(args)\n",
    "\n",
    "mask = data.y[idx_attach] != args.target_class\n",
    "mask = mask.to(device)\n",
    "\n",
    "## only attack those has groud truth labels != target_class ##\n",
    "idx_attach = idx_attach[(data.y[idx_attach] != args.target_class).nonzero().flatten()]\n",
    "\n",
    "bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "# test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "known_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "predictions = []\n",
    "# edge weight for clean edge_index, may use later #\n",
    "edge_weight = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(544, device='cuda:2')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24, 24,  8, 30, 10, 28, 28, 37,  4, 24, 30, 24,  5, 24, 24,  5, 22,  8,\n",
      "         4, 24, 28, 36, 28, 16, 34, 10, 16, 24, 37, 24, 24,  7, 10, 38, 28,  6,\n",
      "        16, 10,  8, 24, 28, 23, 10, 16, 28, 28, 30, 22,  4,  4, 24, 13, 28, 28,\n",
      "         1, 28, 30, 16, 16, 16, 34, 14, 27, 16,  6, 16, 28, 28, 28,  4,  3, 24,\n",
      "        23, 24, 34, 34, 16,  1, 39, 30, 16, 38, 30, 34, 24, 30, 34, 16, 36, 23,\n",
      "        27, 37,  9,  4, 24, 19, 28, 34, 16, 28, 10, 15, 24, 24,  9, 28, 16, 19,\n",
      "         4, 16,  9, 24, 23, 23, 31, 22,  8, 33, 28, 24, 24, 24,  8,  9, 17, 16,\n",
      "        13, 28, 13, 28, 16, 28, 16, 10, 30, 30, 16, 24, 28,  6, 10, 28, 30, 28,\n",
      "        16, 16, 39, 28,  9, 34, 24, 24, 23,  0, 25, 16, 16, 30, 36, 28,  4, 16,\n",
      "         3, 24,  9,  9,  8, 24,  5, 13, 19, 24, 30, 31, 24, 20,  5, 19, 17, 16,\n",
      "        34, 19, 21, 19, 28, 10,  8, 16, 34,  8, 37, 24,  4, 37, 28, 16, 28,  4,\n",
      "        16, 26, 16, 30, 24, 19,  8, 24, 36,  4, 24,  5, 31, 19, 28, 16, 23, 30,\n",
      "         9, 24, 33, 23, 30, 34, 34, 10, 26, 16, 10, 31, 28, 24,  5, 38, 34, 19,\n",
      "        16, 10, 37, 18, 16, 30, 27, 26, 34, 28, 36, 31, 28, 31, 13, 16,  4, 10,\n",
      "         0, 16, 33, 22, 27, 23,  4, 16,  8,  5, 28, 16, 28,  4, 28, 27, 28, 30,\n",
      "        19, 28, 28, 28, 10, 31, 36, 13, 34, 30, 28, 24,  5, 24, 28, 30,  5, 30,\n",
      "         9, 19,  5, 16, 34, 24,  3, 16, 16, 26, 24, 30, 34, 16, 16, 28,  4, 28,\n",
      "         6, 28, 16, 31, 16, 24, 22, 28, 24,  6, 16, 30, 34,  3, 16, 28, 26, 13,\n",
      "        30, 28, 28,  0, 27,  5, 28, 28, 31, 24, 27, 39, 23, 13, 10, 27, 34, 24,\n",
      "        36, 10, 25, 28, 30, 34, 20, 28, 28, 26,  3, 16, 19, 34, 28,  4, 16, 36,\n",
      "        28, 37, 16, 28, 30, 16, 30, 23, 10,  4, 30, 34, 24,  7, 16, 30, 33, 10,\n",
      "        34, 31, 16,  8,  3, 22, 10, 28,  5, 16, 39, 16, 24, 20, 27, 19, 25, 28,\n",
      "         4, 24, 30, 34, 24, 28, 28,  8, 30, 24, 16, 16, 38, 39, 16, 25, 28, 30,\n",
      "        16, 30, 19, 23, 10, 16, 24, 16, 36, 16,  8,  5, 10, 34,  5, 28, 24, 16,\n",
      "        27, 27, 39, 16, 24, 28, 28, 16, 28, 24, 26, 24, 10, 16, 28, 34, 31, 16,\n",
      "        16, 22, 24, 31, 16,  3, 28, 16, 16, 20, 24, 36, 26,  5,  8,  8,  4, 19,\n",
      "        16, 24,  8, 24, 20, 27,  4, 28,  8, 16, 31, 22, 39,  3, 28,  3, 16, 34,\n",
      "        28,  4, 30, 31, 19, 28,  3, 13, 16, 28, 24, 24, 16,  4,  4, 22, 24,  5,\n",
      "        27, 16, 36, 36, 34, 34, 39, 16, 34, 24, 26,  8, 25, 24, 31, 16, 34, 27,\n",
      "        28, 15, 31, 39, 16, 19,  8, 37, 30,  8, 19, 10, 34, 34, 30, 16,  0, 37,\n",
      "         6, 24, 28, 10], device='cuda:2')\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "print(data.y[idx_attach])\n",
    "# idx_attach is selected target node #\n",
    "print(poison_labels[idx_attach])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on poisoned target nodes: 0.9963\n"
     ]
    }
   ],
   "source": [
    "# train a backdoored model on poisoned graph # \n",
    "test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "test_model.fit(poison_x,poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "test_model.eval()\n",
    "clean_acc = test_model.test(poison_x,poison_edge_index, poison_edge_weights,poison_labels,idx_attach)\n",
    "output, x = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "ori_predict = torch.exp(output[known_nodes])\n",
    "print(\"accuracy on poisoned target nodes: {:.4f}\".format(clean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR: 0.9926\n",
      "Flip ASR: 0.9924/16482 nodes\n",
      "CA: 0.6708\n"
     ]
    }
   ],
   "source": [
    "# test backdoored model for comparison #\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "\n",
    "output, x = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "asr = train_attach_rate\n",
    "flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "print(\"CA: {:.4f}\".format(ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge drop #\n",
    "def sample_noise_all(edge_index, edge_weight,device):\n",
    "    noisy_edge_index = edge_index.clone().detach()\n",
    "    if(edge_weight == None):\n",
    "        noisy_edge_weight = torch.ones([noisy_edge_index.shape[1],]).to(device)\n",
    "    else:\n",
    "        noisy_edge_weight = edge_weight.clone().detach()\n",
    "    # # rand_noise_data = copy.deepcopy(data)\n",
    "    # rand_noise_data.edge_weight = torch.ones([rand_noise_data.edge_index.shape[1],]).to(device)\n",
    "    m = Bernoulli(torch.tensor([0.5]).to(device))\n",
    "    mask = m.sample(noisy_edge_weight.shape).squeeze(-1).int()\n",
    "    # print('mask',mask)\n",
    "    rand_inputs = torch.randint_like(noisy_edge_weight, low=0, high=2).squeeze().int().to(device)\n",
    "    # print(rand_noise_data.edge_weight.shape,mask.shape)\n",
    "    noisy_edge_weight = noisy_edge_weight * mask #+ rand_inputs * (1-mask)\n",
    "        \n",
    "    if(noisy_edge_weight!=None):\n",
    "        noisy_edge_index = noisy_edge_index[:,noisy_edge_weight.nonzero().flatten().long()]\n",
    "        noisy_edge_weight = torch.ones([noisy_edge_index.shape[1],]).to(device)\n",
    "    return noisy_edge_index, noisy_edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test robustness #\n",
    "for i in range(20):\n",
    "            test_model.eval()\n",
    "            noisy_poison_edge_index, noisy_poison_edge_weights = sample_noise_all(poison_edge_index, poison_edge_weights, device)\n",
    "            output, x = test_model(poison_x,noisy_poison_edge_index,noisy_poison_edge_weights)\n",
    "            train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "            train_clean_rate = (output.argmax(dim=1)[idx_train]==data.y[idx_train]).float().mean()\n",
    "            predictions.append(torch.exp(output[known_nodes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.argmax(predictions[8][23868])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_of_less_robust tensor([34094, 34307, 34318, 34184, 33984, 34114, 34262, 34390, 34023, 34052,\n",
      "        34381, 34200, 34152, 34189, 34064, 34124, 34117, 34325, 33951, 34055,\n",
      "        33929, 34396, 33892, 34270, 33885, 33977, 34373, 34366, 33902, 34062,\n",
      "        14587, 33998, 33949, 34219, 34405, 33943, 34178, 34071, 34217, 34303,\n",
      "        34134, 34235, 34240, 34175, 34291, 34006, 34047, 34272, 34218, 34283,\n",
      "        34069, 33874, 34216, 33908, 34372, 33868, 34359, 34337, 34327, 34308,\n",
      "        34084, 33897, 34253, 33912, 34106, 34090, 34332, 34128, 33923, 33931,\n",
      "        34059, 33876, 34101, 34146, 34309, 34176, 34394, 34339, 33887, 34321,\n",
      "        34274, 34246, 34027, 34171, 34013, 34316, 33955, 34357, 34228, 34051,\n",
      "        34097, 33979, 34177, 34098, 34085, 33907, 34105, 34314, 34361, 34379,\n",
      "        34140, 34222, 34236, 34132, 34255, 34169, 34155, 34020, 33872, 34407,\n",
      "        34000, 34004, 34192, 34336, 34239, 33937, 34081, 34121, 34209, 34079,\n",
      "        34091, 34302, 34259, 34341, 34377, 34411, 33963, 34151, 34029, 33904,\n",
      "        34285, 33925, 34292, 34057, 34138, 34188, 34315, 34207, 34271, 34273,\n",
      "        34183, 34007, 34194, 34187, 34103, 34213, 33879, 34031, 11406, 34111,\n",
      "        34358, 33896, 34305, 33976, 34354, 34237, 33956, 34048, 34127, 34328,\n",
      "        34205, 33927, 34173, 34364, 34133, 34289, 33986, 33965, 34323, 33922,\n",
      "        33982, 34363, 34230, 33917, 34312, 34296, 33996, 34058, 33880, 34250,\n",
      "        34338, 34350, 34280, 33928, 34014, 33980, 34330, 34391, 34288, 33944,\n",
      "        34258, 34167, 34092, 33960, 33916, 34174, 34242, 33919, 33988, 34159,\n",
      "        33994, 34053, 34311, 34399, 34324, 34153, 33883, 34041, 34370, 34266,\n",
      "        34108, 34034, 34067, 33873, 34263, 34046, 25073, 34063, 34278, 34040,\n",
      "        34382, 34035, 34196, 34406, 34383, 34261, 34287, 33971, 34393, 34276,\n",
      "        34275, 34204, 34021, 33933, 33878, 33895, 34148, 34345, 33974, 34355,\n",
      "        34238, 34147, 34317, 33888, 33920, 34408, 34227, 34269, 34141, 34011,\n",
      "        34367, 34099, 34257, 34072, 34125, 34233, 34282, 34005, 34166, 33952,\n",
      "        34123, 34149, 34186, 33945, 34210, 34162, 34102, 33966, 18417, 33921,\n",
      "        34083, 33995, 34065, 33938, 33926, 34386, 33891, 33989, 34154, 34224,\n",
      "        33870, 34247, 34388, 33914, 34220, 34231, 34076, 34199, 33901, 33930,\n",
      "        34126, 34068, 34089, 34342, 34107, 34313, 16429, 34384,  7659,  7384,\n",
      "        34172, 34136, 34284, 23980, 34351, 34182, 33871, 34033, 33997, 33987,\n",
      "        33935, 34404, 34181, 34249, 34075, 18238, 34028, 33950, 33620, 34335,\n",
      "        34012, 33911, 34334, 34061, 33913, 33947, 34260, 28100, 33903, 33877,\n",
      "        34135, 33918, 34320, 10334, 33939, 34165, 34333, 34304, 34170, 34022,\n",
      "        34137, 20196, 34112, 33970, 34042, 34115, 34158, 34401, 34295, 34201,\n",
      "         8846, 34301, 34410, 34017, 34225, 34088, 34277, 34096, 34049, 34036,\n",
      "        34193, 34104, 21073, 34331, 34113, 33946, 34030, 34298, 34003, 33869,\n",
      "         9411, 34197, 34252, 34300, 33999, 34045, 34019, 25950, 34032, 34389,\n",
      "        34392, 34164,    57, 34221, 34268,  6041, 34054, 34039,  2569, 33898,\n",
      "         1846, 33975, 12900, 18290, 34376, 34402, 33893, 34179, 34232, 18351,\n",
      "        22300, 34362, 34198, 33875, 34375,  8152, 34060, 34160, 28462, 22980,\n",
      "        34116, 11688, 33890, 34243, 34340, 29281, 34349, 29403, 11765, 34385,\n",
      "        33991, 33967, 34254, 34145, 34353, 34234, 30641, 33181, 29139, 33355,\n",
      "        34110, 29355, 34241, 34157, 34109, 34212,  6837, 12040, 34397, 34329,\n",
      "         1508, 34395, 24118, 34156, 34409, 21187, 33957, 33327,  4344, 34142,\n",
      "        34044, 34050, 11545, 34215, 18183, 28601, 33881, 11882, 34025, 34245,\n",
      "         5024, 22264, 34264, 33941, 34129, 13788, 33924, 24161, 34070, 21616,\n",
      "        34037, 33942,  2807, 30259, 34214, 34380, 27746, 33906, 34078,  4464,\n",
      "        34346,  3639,  8405, 30937, 34185, 34286, 21743, 33410, 34265,  7869,\n",
      "         1318,  8677, 34356, 33910, 28936, 31573, 34206, 14755, 30108, 12544,\n",
      "        19084, 24195,  3395, 34009, 32502, 27816,  5986,  4691, 11325, 16585,\n",
      "        34002, 33968, 19307,  7362, 13672, 24317,  4149,  1336,  6270,  4993,\n",
      "         3620, 33983,  2957, 25845,  4502, 17338, 20991,  7358, 28657,  3232,\n",
      "        25905, 15312, 18638, 20209, 33961, 27599, 26299, 27244, 17453,  7779,\n",
      "         5292, 22276, 30571, 29452], device='cuda:2')\n",
      "count 435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zbz5349/anaconda3/envs/spurious/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-8\n",
    "deviations = []\n",
    "for sub_pred in predictions:\n",
    "    sub_pred += epsilon\n",
    "    deviation = F.kl_div(sub_pred.log(), ori_predict, reduce=False)\n",
    "    deviations.append(deviation)\n",
    "\n",
    "summed_deviations = torch.zeros_like(deviations[0]).to(deviations[0].device)\n",
    "for deviation in deviations:\n",
    "    ##### summed deviations for each node #####\n",
    "    summed_deviations += deviation\n",
    "\n",
    "\n",
    "##### get the index for nodes with less robustness #####\n",
    "    \n",
    "##### args.vs_number is unknown #####\n",
    "index_of_less_robust = torch.sort(torch.mean(summed_deviations,dim=-1),descending=True)[1][:mask.sum()]\n",
    "print('index_of_less_robust',index_of_less_robust)\n",
    "\n",
    "##### count how many poisoned target nodes are selected in less robustness nodes #####\n",
    "count = 0\n",
    "dd = []\n",
    "for idx in index_of_less_robust:\n",
    "    if idx >= len(known_nodes)-args.vs_number:\n",
    "        count += 1\n",
    "        dd.append(idx)\n",
    "print('count',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9190, 0.8449, 0.7745, 0.7721, 0.7706, 0.7544, 0.7260, 0.7240, 0.7160,\n",
       "        0.7158, 0.7095, 0.6904, 0.6850, 0.6839, 0.6804, 0.6753, 0.6656, 0.6633,\n",
       "        0.6542, 0.6505, 0.6439, 0.6352, 0.6325, 0.6253, 0.6091, 0.5976, 0.5926,\n",
       "        0.5830, 0.5750, 0.5748, 0.5726, 0.5644, 0.5620, 0.5617, 0.5616, 0.5600,\n",
       "        0.5599, 0.5524, 0.5477, 0.5450, 0.5437, 0.5384, 0.5379, 0.5300, 0.5294,\n",
       "        0.5293, 0.5163, 0.5076, 0.5057, 0.5038, 0.5036, 0.5033, 0.4908, 0.4876,\n",
       "        0.4793, 0.4650, 0.4618, 0.4611, 0.4605, 0.4605, 0.4597, 0.4590, 0.4585,\n",
       "        0.4580, 0.4574, 0.4567, 0.4559, 0.4547, 0.4538, 0.4505, 0.4456, 0.4447,\n",
       "        0.4436, 0.4389, 0.4387, 0.4367, 0.4330, 0.4308, 0.4293, 0.4290, 0.4285,\n",
       "        0.4281, 0.4243, 0.4242, 0.4227, 0.4211, 0.4207, 0.4186, 0.4182, 0.4157,\n",
       "        0.4139, 0.4137, 0.4130, 0.4123, 0.4121, 0.4071, 0.4070, 0.4043, 0.4043,\n",
       "        0.4024, 0.4016, 0.3990, 0.3919, 0.3899, 0.3894, 0.3884, 0.3880, 0.3872,\n",
       "        0.3866, 0.3830, 0.3826, 0.3793, 0.3780, 0.3754, 0.3741, 0.3732, 0.3728,\n",
       "        0.3724, 0.3715, 0.3694, 0.3676, 0.3666, 0.3655, 0.3648, 0.3646, 0.3646,\n",
       "        0.3631, 0.3617, 0.3600, 0.3599, 0.3596, 0.3583, 0.3578, 0.3574, 0.3572,\n",
       "        0.3559, 0.3550, 0.3549, 0.3528, 0.3524, 0.3522, 0.3522, 0.3508, 0.3470,\n",
       "        0.3459, 0.3449, 0.3449, 0.3444, 0.3427, 0.3395, 0.3391, 0.3390, 0.3390,\n",
       "        0.3364, 0.3340, 0.3337, 0.3332, 0.3331, 0.3320, 0.3312, 0.3266, 0.3261,\n",
       "        0.3255, 0.3254, 0.3249, 0.3238, 0.3236, 0.3233, 0.3221, 0.3213, 0.3195,\n",
       "        0.3187, 0.3181, 0.3177, 0.3173, 0.3171, 0.3158, 0.3145, 0.3134, 0.3131,\n",
       "        0.3112, 0.3105, 0.3090, 0.3089, 0.3089, 0.3088, 0.3066, 0.3048, 0.3039,\n",
       "        0.3013, 0.3013, 0.3010, 0.3009, 0.3009, 0.2992, 0.2971, 0.2965, 0.2962,\n",
       "        0.2951, 0.2950, 0.2924, 0.2917, 0.2903, 0.2902, 0.2900, 0.2890, 0.2879,\n",
       "        0.2862, 0.2857, 0.2853, 0.2853, 0.2849, 0.2844, 0.2839, 0.2815, 0.2814,\n",
       "        0.2811, 0.2797, 0.2796, 0.2766, 0.2762, 0.2751, 0.2751, 0.2731, 0.2718,\n",
       "        0.2707, 0.2705, 0.2700, 0.2698, 0.2696, 0.2683, 0.2678, 0.2676, 0.2675,\n",
       "        0.2674, 0.2666, 0.2648, 0.2646, 0.2643, 0.2640, 0.2629, 0.2601, 0.2595,\n",
       "        0.2594, 0.2564, 0.2547, 0.2542, 0.2529, 0.2527, 0.2514, 0.2496, 0.2484,\n",
       "        0.2483, 0.2475, 0.2455, 0.2448, 0.2438, 0.2418, 0.2417, 0.2417, 0.2414,\n",
       "        0.2406, 0.2392, 0.2389, 0.2387, 0.2354, 0.2353, 0.2353, 0.2351, 0.2347,\n",
       "        0.2347, 0.2341, 0.2334, 0.2330, 0.2323, 0.2321, 0.2319, 0.2316, 0.2294,\n",
       "        0.2286, 0.2277, 0.2275, 0.2264, 0.2259, 0.2250, 0.2247, 0.2209, 0.2209,\n",
       "        0.2202, 0.2193, 0.2188, 0.2186, 0.2185, 0.2184, 0.2178, 0.2174, 0.2174,\n",
       "        0.2174, 0.2155, 0.2150, 0.2144, 0.2143, 0.2142, 0.2140, 0.2108, 0.2077,\n",
       "        0.2077, 0.2074, 0.2066, 0.2061, 0.2060, 0.2058, 0.2051, 0.2050, 0.2034,\n",
       "        0.2029, 0.2028, 0.2026, 0.2023, 0.2019, 0.2018, 0.2016, 0.2013, 0.2012,\n",
       "        0.2004, 0.2000, 0.2000, 0.1982, 0.1981, 0.1976, 0.1971, 0.1970, 0.1960,\n",
       "        0.1949, 0.1947, 0.1945, 0.1936, 0.1935, 0.1927, 0.1927, 0.1924, 0.1920,\n",
       "        0.1919, 0.1917, 0.1917, 0.1908, 0.1886, 0.1881, 0.1877, 0.1860, 0.1856,\n",
       "        0.1840, 0.1814, 0.1811, 0.1805, 0.1794, 0.1794, 0.1784, 0.1776, 0.1770,\n",
       "        0.1761, 0.1760, 0.1756, 0.1755, 0.1743, 0.1741, 0.1741, 0.1733, 0.1727,\n",
       "        0.1725, 0.1724, 0.1719, 0.1718, 0.1715, 0.1712, 0.1709, 0.1706, 0.1700,\n",
       "        0.1692, 0.1690, 0.1690, 0.1688, 0.1686, 0.1685, 0.1683, 0.1682, 0.1674,\n",
       "        0.1671, 0.1669, 0.1667, 0.1664, 0.1653, 0.1652, 0.1650, 0.1647, 0.1637,\n",
       "        0.1635, 0.1635, 0.1633, 0.1628, 0.1623, 0.1619, 0.1618, 0.1617, 0.1612,\n",
       "        0.1612, 0.1610, 0.1600, 0.1596, 0.1595, 0.1586, 0.1585, 0.1574, 0.1562,\n",
       "        0.1550, 0.1538, 0.1536, 0.1532, 0.1530, 0.1525, 0.1516, 0.1515, 0.1508,\n",
       "        0.1507, 0.1507, 0.1506, 0.1503, 0.1501, 0.1500, 0.1491, 0.1485, 0.1483,\n",
       "        0.1480, 0.1474, 0.1469, 0.1463, 0.1463, 0.1457, 0.1452, 0.1449, 0.1447,\n",
       "        0.1440, 0.1436, 0.1436, 0.1436, 0.1434, 0.1434, 0.1433, 0.1426, 0.1421,\n",
       "        0.1417, 0.1416, 0.1415, 0.1414, 0.1414, 0.1409, 0.1404, 0.1401, 0.1390,\n",
       "        0.1389, 0.1386, 0.1376, 0.1375, 0.1366, 0.1365, 0.1361, 0.1361, 0.1358,\n",
       "        0.1358, 0.1356, 0.1354, 0.1353, 0.1353, 0.1352, 0.1351, 0.1345, 0.1333,\n",
       "        0.1332, 0.1331, 0.1320, 0.1318, 0.1310, 0.1302, 0.1300, 0.1297, 0.1294,\n",
       "        0.1293, 0.1283, 0.1279, 0.1269, 0.1269, 0.1268, 0.1266, 0.1266, 0.1265,\n",
       "        0.1263, 0.1261, 0.1259, 0.1259, 0.1257, 0.1255, 0.1252, 0.1250, 0.1250,\n",
       "        0.1249, 0.1240, 0.1239, 0.1230, 0.1226, 0.1226, 0.1226, 0.1207, 0.1206,\n",
       "        0.1206, 0.1199, 0.1196, 0.1189, 0.1185, 0.1184, 0.1180, 0.1179, 0.1176,\n",
       "        0.1167, 0.1162, 0.1162, 0.1161, 0.1160, 0.1159, 0.1158, 0.1157, 0.1155,\n",
       "        0.1154, 0.1152, 0.1151, 0.1150, 0.1148, 0.1147, 0.1146, 0.1146, 0.1143,\n",
       "        0.1142, 0.1138, 0.1137, 0.1137], device='cuda:2',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sort(torch.mean(summed_deviations,dim=-1),descending=True)[0][:mask.sum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(544, device='cuda:2')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ori_predict))\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# visualize robustness distribution #\n",
    "a=torch.mean(summed_deviations,dim=-1)\n",
    "a_excluding_last_40 = a[:-args.vs_number].detach().cpu()\n",
    "a_last=a[-mask.sum():].detach().cpu()\n",
    "\n",
    "\n",
    "a_last_40 = a[-args.vs_number:].detach().cpu()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "indices_to_remove = [i for i, node in enumerate(known_nodes[-args.vs_number:]) if data.y[node] == args.target_class]\n",
    "\n",
    "indices_to_remove = []\n",
    "\n",
    "a_last_list = list(a_last)\n",
    "\n",
    "indices_to_check = range(len(known_nodes[-args.vs_number:]))\n",
    "\n",
    "# indices_to_remove = []\n",
    "\n",
    "# ##### find those nodes that are already have target class or poisoned failed #####\n",
    "# for i in indices_to_check:\n",
    "#     condition1 = data.y[known_nodes[-args.vs_number:][i]] == args.target_class\n",
    "#     condition2 = output.argmax(dim=1)[idx_attach[i]] != args.target_class\n",
    "    \n",
    "#     if condition1 or condition2:\n",
    "#         indices_to_remove.append(i)\n",
    "\n",
    "# for index in sorted(indices_to_remove, reverse=True):\n",
    "#     del a_last_list[index]\n",
    "\n",
    "\n",
    "a_last_40 = a_last_list\n",
    "plt.figure(figsize=(10, 6))\n",
    "# plt.figure(figsize=(40, 24))\n",
    "##### benign nodes #####\n",
    "plt.hist(a_excluding_last_40, bins=20, alpha=0.5, label='benign nodes', density=True)\n",
    "##### poisoned success nodes #####\n",
    "plt.hist(a_last_40, bins=20, alpha=0.5, label='poisoned target nodes',density=True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of robustness for each node')\n",
    "plt.savefig('a.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1471694"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_edge_index[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([169343, 169343, 169343, 169346, 169346, 169346, 169349, 169349, 169349,\n",
      "        169352, 169352, 169352, 169355, 169355, 169355, 169358, 169358, 169358,\n",
      "        169361, 169361, 169361, 169364, 169364, 169364, 169367, 169367, 169367,\n",
      "        169370, 169370, 169370, 169373, 169373, 169373, 169376, 169376, 169376,\n",
      "        169379, 169379, 169379, 169382, 169382, 169382, 169385, 169385, 169385,\n",
      "        169388, 169388, 169388, 169391, 169391, 169391, 169394, 169394, 169394,\n",
      "        169397, 169397, 169397, 169400, 169400, 169400, 169403, 169403, 169403,\n",
      "        169406, 169406, 169406, 169409, 169409, 169409, 169412, 169412, 169412,\n",
      "        169415, 169415, 169415, 169418, 169418, 169418, 169421, 169421, 169421,\n",
      "        169424, 169424, 169424, 169427, 169427, 169427, 169430, 169430, 169430,\n",
      "        169433, 169433, 169433, 169436, 169436, 169436, 169439, 169439, 169439,\n",
      "        169442, 169442, 169442, 169445, 169445, 169445, 169448, 169448, 169448,\n",
      "        169451, 169451, 169451, 169454, 169454, 169454, 169457, 169457, 169457,\n",
      "        169460, 169460, 169460, 169463, 169463, 169463, 169466, 169466, 169466,\n",
      "        169469, 169469, 169469, 169472, 169472, 169472, 169475, 169475, 169475,\n",
      "        169478, 169478, 169478, 169481, 169481, 169481, 169484, 169484, 169484,\n",
      "        169487, 169487, 169487, 169490, 169490, 169490, 169493, 169493, 169493,\n",
      "        169496, 169496, 169496, 169499, 169499, 169499, 169502, 169502, 169502,\n",
      "        169505, 169505, 169505, 169508, 169508, 169508, 169511, 169511, 169511,\n",
      "        169514, 169514, 169514, 169517, 169517, 169517, 169520, 169520, 169520,\n",
      "        169523, 169523, 169523, 169526, 169526, 169526, 169529, 169529, 169529,\n",
      "        169532, 169532, 169532, 169535, 169535, 169535, 169538, 169538, 169538,\n",
      "        169541, 169541, 169541, 169544, 169544, 169544, 169547, 169547, 169547,\n",
      "        169550, 169550, 169550, 169553, 169553, 169553, 169556, 169556, 169556,\n",
      "        169559, 169559, 169559, 169562, 169562, 169562, 169565, 169565, 169565,\n",
      "        169568, 169568, 169568, 169571, 169571, 169571, 169574, 169574, 169574,\n",
      "        169577, 169577, 169577, 169580, 169580, 169580, 169583, 169583, 169583,\n",
      "        169586, 169586, 169586, 169589, 169589, 169589, 169592, 169592, 169592,\n",
      "        169595, 169595, 169595, 169598, 169598, 169598, 169601, 169601, 169601,\n",
      "        169604, 169604, 169604, 169607, 169607, 169607, 169610, 169610, 169610,\n",
      "        169613, 169613, 169613, 169616, 169616, 169616, 169619, 169619, 169619,\n",
      "        169622, 169622, 169622, 169625, 169625, 169625, 169628, 169628, 169628,\n",
      "        169631, 169631, 169631, 169634, 169634, 169634, 169637, 169637, 169637,\n",
      "        169640, 169640, 169640, 169643, 169643, 169643, 169646, 169646, 169646,\n",
      "        169649, 169649, 169649, 169652, 169652, 169652, 169655, 169655, 169655,\n",
      "        169658, 169658, 169658, 169661, 169661, 169661, 169664, 169664, 169664,\n",
      "        169667, 169667, 169667, 169670, 169670, 169670, 169673, 169673, 169673,\n",
      "        169676, 169676, 169676, 169679, 169679, 169679, 169682, 169682, 169682,\n",
      "        169685, 169685, 169685, 169688, 169688, 169688, 169691, 169691, 169691,\n",
      "        169694, 169694, 169694, 169697, 169697, 169697, 169700, 169700, 169700,\n",
      "        169703, 169703, 169703, 169706, 169706, 169706, 169709, 169709, 169709,\n",
      "        169712, 169712, 169712, 169715, 169715, 169715, 169718, 169718, 169718,\n",
      "        169721, 169721, 169721, 169724, 169724, 169724, 169727, 169727, 169727,\n",
      "        169730, 169730, 169730, 169733, 169733, 169733, 169736, 169736, 169736,\n",
      "        169739, 169739, 169739, 169742, 169742, 169742, 169745, 169745, 169745,\n",
      "        169748, 169748, 169748, 169751, 169751, 169751, 169754, 169754, 169754,\n",
      "        169757, 169757, 169757, 169760, 169760, 169760, 169763, 169763, 169763,\n",
      "        169766, 169766, 169766, 169769, 169769, 169769, 169772, 169772, 169772,\n",
      "        169775, 169775, 169775, 169778, 169778, 169778, 169781, 169781, 169781,\n",
      "        169784, 169784, 169784, 169787, 169787, 169787, 169790, 169790, 169790,\n",
      "        169793, 169793, 169793, 169796, 169796, 169796, 169799, 169799, 169799,\n",
      "        169802, 169802, 169802, 169805, 169805, 169805, 169808, 169808, 169808,\n",
      "        169811, 169811, 169811, 169814, 169814, 169814, 169817, 169817, 169817,\n",
      "        169820, 169820, 169820, 169823, 169823, 169823, 169826, 169826, 169826,\n",
      "        169829, 169829, 169829, 169832, 169832, 169832, 169835, 169835, 169835,\n",
      "        169838, 169838, 169838, 169841, 169841, 169841, 169844, 169844, 169844,\n",
      "        169847, 169847, 169847, 169850, 169850, 169850, 169853, 169853, 169853,\n",
      "        169856, 169856, 169856, 169859, 169859, 169859, 169862, 169862, 169862,\n",
      "        169865, 169865, 169865, 169868, 169868, 169868, 169871, 169871, 169871,\n",
      "        169874, 169874, 169874, 169877, 169877, 169877, 169880, 169880, 169880,\n",
      "        169883, 169883, 169883, 169886, 169886, 169886, 169889, 169889, 169889,\n",
      "        169892, 169892, 169892, 169895, 169895, 169895, 169898, 169898, 169898,\n",
      "        169901, 169901, 169901, 169904, 169904, 169904, 169907, 169907, 169907,\n",
      "        169910, 169910, 169910, 169913, 169913, 169913, 169916, 169916, 169916,\n",
      "        169919, 169919, 169919, 169922, 169922, 169922, 169925, 169925, 169925,\n",
      "        169928, 169928, 169928, 169931, 169931, 169931, 169934, 169934, 169934,\n",
      "        169937, 169937, 169937, 169940, 169940, 169940, 169943, 169943, 169943,\n",
      "        169946, 169946, 169946, 169949, 169949, 169949, 169952, 169952, 169952,\n",
      "        169955, 169955, 169955, 169958, 169958, 169958, 169961, 169961, 169961,\n",
      "        169964, 169964, 169964, 169967, 169967, 169967, 169970, 169970, 169970,\n",
      "        169973, 169973, 169973, 169976, 169976, 169976, 169979, 169979, 169979,\n",
      "        169982, 169982, 169982, 169985, 169985, 169985, 169988, 169988, 169988,\n",
      "        169991, 169991, 169991, 169994, 169994, 169994, 169997, 169997, 169997,\n",
      "        170000, 170000, 170000, 170003, 170003, 170003, 170006, 170006, 170006,\n",
      "        170009, 170009, 170009, 170012, 170012, 170012, 170015, 170015, 170015,\n",
      "        170018, 170018, 170018, 170021, 170021, 170021, 170024, 170024, 170024,\n",
      "        170027, 170027, 170027, 170030, 170030, 170030, 170033, 170033, 170033,\n",
      "        170036, 170036, 170036, 170039, 170039, 170039, 170042, 170042, 170042,\n",
      "        170045, 170045, 170045, 170048, 170048, 170048, 170051, 170051, 170051,\n",
      "        170054, 170054, 170054, 170057, 170057, 170057, 170060, 170060, 170060,\n",
      "        170063, 170063, 170063, 170066, 170066, 170066, 170069, 170069, 170069,\n",
      "        170072, 170072, 170072, 170075, 170075, 170075, 170078, 170078, 170078,\n",
      "        170081, 170081, 170081, 170084, 170084, 170084, 170087, 170087, 170087,\n",
      "        170090, 170090, 170090, 170093, 170093, 170093, 170096, 170096, 170096,\n",
      "        170099, 170099, 170099, 170102, 170102, 170102, 170105, 170105, 170105,\n",
      "        170108, 170108, 170108, 170111, 170111, 170111, 170114, 170114, 170114,\n",
      "        170117, 170117, 170117, 170120, 170120, 170120, 170123, 170123, 170123,\n",
      "        170126, 170126, 170126, 170129, 170129, 170129, 170132, 170132, 170132,\n",
      "        170135, 170135, 170135, 170138, 170138, 170138, 170141, 170141, 170141,\n",
      "        170144, 170144, 170144, 170147, 170147, 170147, 170150, 170150, 170150,\n",
      "        170153, 170153, 170153, 170156, 170156, 170156, 170159, 170159, 170159,\n",
      "        170162, 170162, 170162, 170165, 170165, 170165, 170168, 170168, 170168,\n",
      "        170171, 170171, 170171, 170174, 170174, 170174, 170177, 170177, 170177,\n",
      "        170180, 170180, 170180, 170183, 170183, 170183, 170186, 170186, 170186,\n",
      "        170189, 170189, 170189, 170192, 170192, 170192, 170195, 170195, 170195,\n",
      "        170198, 170198, 170198, 170201, 170201, 170201, 170204, 170204, 170204,\n",
      "        170207, 170207, 170207, 170210, 170210, 170210, 170213, 170213, 170213,\n",
      "        170216, 170216, 170216, 170219, 170219, 170219, 170222, 170222, 170222,\n",
      "        170225, 170225, 170225, 170228, 170228, 170228, 170231, 170231, 170231,\n",
      "        170234, 170234, 170234, 170237, 170237, 170237, 170240, 170240, 170240,\n",
      "        170243, 170243, 170243, 170246, 170246, 170246, 170249, 170249, 170249,\n",
      "        170252, 170252, 170252, 170255, 170255, 170255, 170258, 170258, 170258,\n",
      "        170261, 170261, 170261, 170264, 170264, 170264, 170267, 170267, 170267,\n",
      "        170270, 170270, 170270, 170273, 170273, 170273, 170276, 170276, 170276,\n",
      "        170279, 170279, 170279, 170282, 170282, 170282, 170285, 170285, 170285,\n",
      "        170288, 170288, 170288, 170291, 170291, 170291, 170294, 170294, 170294,\n",
      "        170297, 170297, 170297, 170300, 170300, 170300, 170303, 170303, 170303,\n",
      "        170306, 170306, 170306, 170309, 170309, 170309, 170312, 170312, 170312,\n",
      "        170315, 170315, 170315, 170318, 170318, 170318, 170321, 170321, 170321,\n",
      "        170324, 170324, 170324, 170327, 170327, 170327, 170330, 170330, 170330,\n",
      "        170333, 170333, 170333, 170336, 170336, 170336, 170339, 170339, 170339,\n",
      "        170342, 170342, 170342, 170345, 170345, 170345, 170348, 170348, 170348,\n",
      "        170351, 170351, 170351, 170354, 170354, 170354, 170357, 170357, 170357,\n",
      "        170360, 170360, 170360, 170363, 170363, 170363, 170366, 170366, 170366,\n",
      "        170369, 170369, 170369, 170372, 170372, 170372, 170375, 170375, 170375,\n",
      "        170378, 170378, 170378, 170381, 170381, 170381, 170384, 170384, 170384,\n",
      "        170387, 170387, 170387, 170390, 170390, 170390, 170393, 170393, 170393,\n",
      "        170396, 170396, 170396, 170399, 170399, 170399, 170402, 170402, 170402,\n",
      "        170405, 170405, 170405, 170408, 170408, 170408, 170411, 170411, 170411,\n",
      "        170414, 170414, 170414, 170417, 170417, 170417, 170420, 170420, 170420,\n",
      "        170423, 170423, 170423, 170426, 170426, 170426, 170429, 170429, 170429,\n",
      "        170432, 170432, 170432, 170435, 170435, 170435, 170438, 170438, 170438,\n",
      "        170441, 170441, 170441, 170444, 170444, 170444, 170447, 170447, 170447,\n",
      "        170450, 170450, 170450, 170453, 170453, 170453, 170456, 170456, 170456,\n",
      "        170459, 170459, 170459, 170462, 170462, 170462, 170465, 170465, 170465,\n",
      "        170468, 170468, 170468, 170471, 170471, 170471, 170474, 170474, 170474,\n",
      "        170477, 170477, 170477, 170480, 170480, 170480, 170483, 170483, 170483,\n",
      "        170486, 170486, 170486, 170489, 170489, 170489, 170492, 170492, 170492,\n",
      "        170495, 170495, 170495, 170498, 170498, 170498, 170501, 170501, 170501,\n",
      "        170504, 170504, 170504, 170507, 170507, 170507, 170510, 170510, 170510,\n",
      "        170513, 170513, 170513, 170516, 170516, 170516, 170519, 170519, 170519,\n",
      "        170522, 170522, 170522, 170525, 170525, 170525, 170528, 170528, 170528,\n",
      "        170531, 170531, 170531, 170534, 170534, 170534, 170537, 170537, 170537,\n",
      "        170540, 170540, 170540, 170543, 170543, 170543, 170546, 170546, 170546,\n",
      "        170549, 170549, 170549, 170552, 170552, 170552, 170555, 170555, 170555,\n",
      "        170558, 170558, 170558, 170561, 170561, 170561, 170564, 170564, 170564,\n",
      "        170567, 170567, 170567, 170570, 170570, 170570, 170573, 170573, 170573,\n",
      "        170576, 170576, 170576, 170579, 170579, 170579, 170582, 170582, 170582,\n",
      "        170585, 170585, 170585, 170588, 170588, 170588, 170591, 170591, 170591,\n",
      "        170594, 170594, 170594, 170597, 170597, 170597, 170600, 170600, 170600,\n",
      "        170603, 170603, 170603, 170606, 170606, 170606, 170609, 170609, 170609,\n",
      "        170612, 170612, 170612, 170615, 170615, 170615, 170618, 170618, 170618,\n",
      "        170621, 170621, 170621, 170624, 170624, 170624, 170627, 170627, 170627,\n",
      "        170630, 170630, 170630, 170633, 170633, 170633, 170636, 170636, 170636,\n",
      "        170639, 170639, 170639, 170642, 170642, 170642, 170645, 170645, 170645,\n",
      "        170648, 170648, 170648, 170651, 170651, 170651, 170654, 170654, 170654,\n",
      "        170657, 170657, 170657, 170660, 170660, 170660, 170663, 170663, 170663,\n",
      "        170666, 170666, 170666, 170669, 170669, 170669, 170672, 170672, 170672,\n",
      "        170675, 170675, 170675, 170678, 170678, 170678, 170681, 170681, 170681,\n",
      "        170684, 170684, 170684, 170687, 170687, 170687, 170690, 170690, 170690,\n",
      "        170693, 170693, 170693, 170696, 170696, 170696, 170699, 170699, 170699,\n",
      "        170702, 170702, 170702, 170705, 170705, 170705, 170708, 170708, 170708,\n",
      "        170711, 170711, 170711, 170714, 170714, 170714, 170717, 170717, 170717,\n",
      "        170720, 170720, 170720, 170723, 170723, 170723, 170726, 170726, 170726,\n",
      "        170729, 170729, 170729, 170732, 170732, 170732, 170735, 170735, 170735,\n",
      "        170738, 170738, 170738, 170741, 170741, 170741, 170744, 170744, 170744,\n",
      "        170747, 170747, 170747, 170750, 170750, 170750, 170753, 170753, 170753,\n",
      "        170756, 170756, 170756, 170759, 170759, 170759, 170762, 170762, 170762,\n",
      "        170765, 170765, 170765, 170768, 170768, 170768, 170771, 170771, 170771,\n",
      "        170774, 170774, 170774, 170777, 170777, 170777, 170780, 170780, 170780,\n",
      "        170783, 170783, 170783, 170786, 170786, 170786, 170789, 170789, 170789,\n",
      "        170792, 170792, 170792, 170795, 170795, 170795, 170798, 170798, 170798,\n",
      "        170801, 170801, 170801, 170804, 170804, 170804, 170807, 170807, 170807,\n",
      "        170810, 170810, 170810, 170813, 170813, 170813, 170816, 170816, 170816,\n",
      "        170819, 170819, 170819, 170822, 170822, 170822, 170825, 170825, 170825,\n",
      "        170828, 170828, 170828, 170831, 170831, 170831, 170834, 170834, 170834,\n",
      "        170837, 170837, 170837, 170840, 170840, 170840, 170843, 170843, 170843,\n",
      "        170846, 170846, 170846, 170849, 170849, 170849, 170852, 170852, 170852,\n",
      "        170855, 170855, 170855, 170858, 170858, 170858, 170861, 170861, 170861,\n",
      "        170864, 170864, 170864, 170867, 170867, 170867, 170870, 170870, 170870,\n",
      "        170873, 170873, 170873, 170876, 170876, 170876, 170879, 170879, 170879,\n",
      "        170882, 170882, 170882, 170885, 170885, 170885, 170888, 170888, 170888,\n",
      "        170891, 170891, 170891, 170894, 170894, 170894, 170897, 170897, 170897,\n",
      "        170900, 170900, 170900, 170903, 170903, 170903, 170906, 170906, 170906,\n",
      "        170909, 170909, 170909, 170912, 170912, 170912, 170915, 170915, 170915,\n",
      "        170918, 170918, 170918, 170921, 170921, 170921, 170924, 170924, 170924,\n",
      "        170927, 170927, 170927, 170930, 170930, 170930, 170933, 170933, 170933,\n",
      "        170936, 170936, 170936, 170939, 170939, 170939, 170942, 170942, 170942,\n",
      "        170945, 170945, 170945, 170948, 170948, 170948, 170951, 170951, 170951,\n",
      "        170954, 170954, 170954, 170957, 170957, 170957, 170960, 170960, 170960,\n",
      "        170963, 170963, 170963, 170966, 170966, 170966, 170969, 170969, 170969,\n",
      "        170972, 170972, 170972, 170975, 170975, 170975, 170978, 170978, 170978,\n",
      "        170981, 170981, 170981, 170984, 170984, 170984, 170987, 170987, 170987,\n",
      "        170990, 170990, 170990, 170993, 170993, 170993, 170996, 170996, 170996,\n",
      "        170999, 170999, 170999, 171002, 171002, 171002, 171005, 171005, 171005,\n",
      "        171008, 171008, 171008, 171011, 171011, 171011, 171014, 171014, 171014,\n",
      "        171017, 171017, 171017, 171020, 171020, 171020, 171023, 171023, 171023,\n",
      "        171026, 171026, 171026, 171029, 171029, 171029, 171032, 171032, 171032,\n",
      "        171035, 171035, 171035, 151986, 169344, 169345, 162467, 169347, 169348,\n",
      "         44926, 169350, 169351,  23940, 169353, 169354,  72501, 169356, 169357,\n",
      "         13300, 169359, 169360,  22065, 169362, 169363, 147153, 169365, 169366,\n",
      "         79242, 169368, 169369, 128143, 169371, 169372,  93465, 169374, 169375,\n",
      "         63507, 169377, 169378,  73972, 169380, 169381,  73044, 169383, 169384,\n",
      "         56937, 169386, 169387,  88754, 169389, 169390, 163842, 169392, 169393,\n",
      "        119362, 169395, 169396,  54554, 169398, 169399,  33519, 169401, 169402,\n",
      "          8154, 169404, 169405,  16049, 169407, 169408, 113091, 169410, 169411,\n",
      "        156331, 169413, 169414,  35927, 169416, 169417, 152431, 169419, 169420,\n",
      "         38080, 169422, 169423, 168947, 169425, 169426, 121840, 169428, 169429,\n",
      "        101165, 169431, 169432,  34467, 169434, 169435, 130677, 169437, 169438,\n",
      "        127703, 169440, 169441, 162722, 169443, 169444,   4875, 169446, 169447,\n",
      "         95491, 169449, 169450, 148529, 169452, 169453,  78072, 169455, 169456,\n",
      "        126281, 169458, 169459,  59601, 169461, 169462,  54103, 169464, 169465,\n",
      "        107477, 169467, 169468,  51951, 169470, 169471,  10138, 169473, 169474,\n",
      "         10104, 169476, 169477,  26393, 169479, 169480,  18876, 169482, 169483,\n",
      "        151911, 169485, 169486,  73699, 169488, 169489, 122090, 169491, 169492,\n",
      "        149541, 169494, 169495,   7918, 169497, 169498,  23625, 169500, 169501,\n",
      "         54054, 169503, 169504, 165054, 169506, 169507,  90478, 169509, 169510,\n",
      "        130766, 169512, 169513, 106047, 169515, 169516,  62056, 169518, 169519,\n",
      "        152543, 169521, 169522, 100119, 169524, 169525,  62343, 169527, 169528,\n",
      "        119437, 169530, 169531, 129228, 169533, 169534, 130514, 169536, 169537,\n",
      "         12459, 169539, 169540,  58483, 169542, 169543,  35161, 169545, 169546,\n",
      "         27651, 169548, 169549,  77539, 169551, 169552,  49360, 169554, 169555,\n",
      "         43087, 169557, 169558,  51852, 169560, 169561,  84642, 169563, 169564,\n",
      "        162577, 169566, 169567, 108656, 169569, 169570,  81292, 169572, 169573,\n",
      "         49283, 169575, 169576,  70860, 169578, 169579,  49404, 169581, 169582,\n",
      "        141270, 169584, 169585,  36926, 169587, 169588, 101724, 169590, 169591,\n",
      "         14727, 169593, 169594,  12525, 169596, 169597,  19995, 169599, 169600,\n",
      "        162207, 169602, 169603,  69782, 169605, 169606, 127237, 169608, 169609,\n",
      "          8541, 169611, 169612,  83147, 169614, 169615, 131520, 169617, 169618,\n",
      "         97803, 169620, 169621, 109806, 169623, 169624, 145193, 169626, 169627,\n",
      "        118680, 169629, 169630,  20924, 169632, 169633, 131955, 169635, 169636,\n",
      "        106455, 169638, 169639, 129401, 169641, 169642,  58891, 169644, 169645,\n",
      "        116300, 169647, 169648,  34079, 169650, 169651,    224, 169653, 169654,\n",
      "          3172, 169656, 169657,  88160, 169659, 169660, 109185, 169662, 169663,\n",
      "        110668, 169665, 169666, 103248, 169668, 169669,  95078, 169671, 169672,\n",
      "        154019, 169674, 169675, 168817, 169677, 169678,   3771, 169680, 169681,\n",
      "         18207, 169683, 169684, 154860, 169686, 169687,  26834, 169689, 169690,\n",
      "         11175, 169692, 169693,  63122, 169695, 169696,  47747, 169698, 169699,\n",
      "         80305, 169701, 169702, 102581, 169704, 169705,  82265, 169707, 169708,\n",
      "        117214, 169710, 169711, 132492, 169713, 169714, 137535, 169716, 169717,\n",
      "        122891, 169719, 169720, 164548, 169722, 169723, 100623, 169725, 169726,\n",
      "         17308, 169728, 169729, 153845, 169731, 169732,  91474, 169734, 169735,\n",
      "        142746, 169737, 169738,  85505, 169740, 169741,  97061, 169743, 169744,\n",
      "        152602, 169746, 169747, 116420, 169749, 169750,  15941, 169752, 169753,\n",
      "        152529, 169755, 169756,  19807, 169758, 169759,  15146, 169761, 169762,\n",
      "        148629, 169764, 169765, 162255, 169767, 169768, 138185, 169770, 169771,\n",
      "         96655, 169773, 169774,  71909, 169776, 169777,  25179, 169779, 169780,\n",
      "         34595, 169782, 169783, 147363, 169785, 169786,  64609, 169788, 169789,\n",
      "         67502, 169791, 169792,   8161, 169794, 169795, 136765, 169797, 169798,\n",
      "        110630, 169800, 169801,  11895, 169803, 169804, 158529, 169806, 169807,\n",
      "          5181, 169809, 169810,  90008, 169812, 169813,  16843, 169815, 169816,\n",
      "         62749, 169818, 169819,  33553, 169821, 169822,  15212, 169824, 169825,\n",
      "         24861, 169827, 169828, 160062, 169830, 169831,  72270, 169833, 169834,\n",
      "         25230, 169836, 169837,  76515, 169839, 169840,  27120, 169842, 169843,\n",
      "        152018, 169845, 169846,  95828, 169848, 169849, 124173, 169851, 169852,\n",
      "         80523, 169854, 169855,  28845, 169857, 169858,  84596, 169860, 169861,\n",
      "         56397, 169863, 169864,     63, 169866, 169867, 164765, 169869, 169870,\n",
      "        140751, 169872, 169873,  37912, 169875, 169876,  47903, 169878, 169879,\n",
      "         69429, 169881, 169882, 132506, 169884, 169885,  66631, 169887, 169888,\n",
      "        153695, 169890, 169891,  61748, 169893, 169894,  18334, 169896, 169897,\n",
      "         94547, 169899, 169900,  97448, 169902, 169903,  58066, 169905, 169906,\n",
      "          1646, 169908, 169909, 158386, 169911, 169912, 153884, 169914, 169915,\n",
      "         39628, 169917, 169918, 129569, 169920, 169921,  86483, 169923, 169924,\n",
      "        159823, 169926, 169927, 109138, 169929, 169930,  52390, 169932, 169933,\n",
      "         20765, 169935, 169936,  70732, 169938, 169939,  56531, 169941, 169942,\n",
      "         14090, 169944, 169945, 144949, 169947, 169948,  45603, 169950, 169951,\n",
      "        136798, 169953, 169954, 162265, 169956, 169957,  66578, 169959, 169960,\n",
      "         42077, 169962, 169963, 144225, 169965, 169966,  23731, 169968, 169969,\n",
      "         37597, 169971, 169972,  73034, 169974, 169975, 115402, 169977, 169978,\n",
      "         40876, 169980, 169981, 104645, 169983, 169984, 123502, 169986, 169987,\n",
      "        154395, 169989, 169990,  47564, 169992, 169993, 112664, 169995, 169996,\n",
      "        137399, 169998, 169999,  62430, 170001, 170002,   6464, 170004, 170005,\n",
      "        124696, 170007, 170008,  50606, 170010, 170011,  34820, 170013, 170014,\n",
      "        116196, 170016, 170017, 136597, 170019, 170020, 145708, 170022, 170023,\n",
      "        105880, 170025, 170026, 129765, 170028, 170029,  57303, 170031, 170032,\n",
      "        155369, 170034, 170035, 114620, 170037, 170038,  95339, 170040, 170041,\n",
      "        103305, 170043, 170044,  83091, 170046, 170047,  68857, 170049, 170050,\n",
      "         73046, 170052, 170053,   3183, 170055, 170056, 149185, 170058, 170059,\n",
      "         80135, 170061, 170062, 155473, 170064, 170065, 164332, 170067, 170068,\n",
      "         20333, 170070, 170071, 139918, 170073, 170074,  45024, 170076, 170077,\n",
      "        141475, 170079, 170080,  91437, 170082, 170083,   2599, 170085, 170086,\n",
      "         74708, 170088, 170089, 160785, 170091, 170092, 168601, 170094, 170095,\n",
      "          2915, 170097, 170098,  75096, 170100, 170101,   5486, 170103, 170104,\n",
      "         69934, 170106, 170107,   3244, 170109, 170110, 108649, 170112, 170113,\n",
      "         69213, 170115, 170116,  96039, 170118, 170119,  74594, 170121, 170122,\n",
      "        144675, 170124, 170125,  65112, 170127, 170128, 157016, 170130, 170131,\n",
      "        150964, 170133, 170134,  91340, 170136, 170137,  17814, 170139, 170140,\n",
      "        134345, 170142, 170143,  74006, 170145, 170146,  51257, 170148, 170149,\n",
      "        159074, 170151, 170152, 165012, 170154, 170155,  78616, 170157, 170158,\n",
      "         87458, 170160, 170161,  26847, 170163, 170164, 138007, 170166, 170167,\n",
      "        123722, 170169, 170170,  44990, 170172, 170173,  70164, 170175, 170176,\n",
      "        162681, 170178, 170179, 155937, 170181, 170182,  70939, 170184, 170185,\n",
      "        115374, 170187, 170188, 166985, 170190, 170191, 112764, 170193, 170194,\n",
      "        144479, 170196, 170197,  26546, 170199, 170200,  53170, 170202, 170203,\n",
      "         25088, 170205, 170206,  22346, 170208, 170209, 157904, 170211, 170212,\n",
      "         75119, 170214, 170215,  34364, 170217, 170218,  98424, 170220, 170221,\n",
      "         93012, 170223, 170224, 118674, 170226, 170227,  89130, 170229, 170230,\n",
      "         72419, 170232, 170233,  29984, 170235, 170236, 117334, 170238, 170239,\n",
      "         16483, 170241, 170242,  19802, 170244, 170245, 113586, 170247, 170248,\n",
      "         92262, 170250, 170251, 140284, 170253, 170254, 157255, 170256, 170257,\n",
      "        148016, 170259, 170260,  96550, 170262, 170263,  35893, 170265, 170266,\n",
      "         73545, 170268, 170269,  38199, 170271, 170272,  60664, 170274, 170275,\n",
      "          1496, 170277, 170278, 158258, 170280, 170281, 135970, 170283, 170284,\n",
      "        121417, 170286, 170287, 141379, 170289, 170290, 164731, 170292, 170293,\n",
      "        118159, 170295, 170296,  29576, 170298, 170299,  75840, 170301, 170302,\n",
      "        144213, 170304, 170305,  73692, 170307, 170308, 104518, 170310, 170311,\n",
      "        103593, 170313, 170314, 126236, 170316, 170317, 108326, 170319, 170320,\n",
      "         89873, 170322, 170323, 140364, 170325, 170326,  97116, 170328, 170329,\n",
      "        124288, 170331, 170332, 121316, 170334, 170335,   5171, 170337, 170338,\n",
      "        152926, 170340, 170341,   3575, 170343, 170344,  68393, 170346, 170347,\n",
      "        168823, 170349, 170350, 134560, 170352, 170353,  97522, 170355, 170356,\n",
      "        147712, 170358, 170359, 152664, 170361, 170362,  14128, 170364, 170365,\n",
      "          3127, 170367, 170368,  20516, 170370, 170371,  85926, 170373, 170374,\n",
      "         13117, 170376, 170377, 122575, 170379, 170380,  59647, 170382, 170383,\n",
      "        147988, 170385, 170386,  15855, 170388, 170389,  38123, 170391, 170392,\n",
      "         72738, 170394, 170395, 123088, 170397, 170398,  28674, 170400, 170401,\n",
      "        137800, 170403, 170404, 151853, 170406, 170407,  40493, 170409, 170410,\n",
      "        119849, 170412, 170413, 125141, 170415, 170416,  28030, 170418, 170419,\n",
      "         91051, 170421, 170422, 137817, 170424, 170425,  32725, 170427, 170428,\n",
      "        151387, 170430, 170431,  23191, 170433, 170434, 123924, 170436, 170437,\n",
      "         94511, 170439, 170440, 140409, 170442, 170443,  84661, 170445, 170446,\n",
      "         57390, 170448, 170449, 108036, 170451, 170452, 146481, 170454, 170455,\n",
      "         50713, 170457, 170458,  25635, 170460, 170461,   8406, 170463, 170464,\n",
      "         53176, 170466, 170467,  63767, 170469, 170470,  10085, 170472, 170473,\n",
      "         11511, 170475, 170476,  10917, 170478, 170479, 125420, 170481, 170482,\n",
      "        106500, 170484, 170485, 122872, 170487, 170488, 131682, 170490, 170491,\n",
      "        136815, 170493, 170494,  70797, 170496, 170497,   9775, 170499, 170500,\n",
      "        107485, 170502, 170503,  34616, 170505, 170506,  38800, 170508, 170509,\n",
      "        157312, 170511, 170512, 121295, 170514, 170515, 144843, 170517, 170518,\n",
      "         53058, 170520, 170521,  86092, 170523, 170524,  78410, 170526, 170527,\n",
      "        129884, 170529, 170530, 122432, 170532, 170533,  34859, 170535, 170536,\n",
      "         20803, 170538, 170539,  68352, 170541, 170542,  62509, 170544, 170545,\n",
      "         56748, 170547, 170548,  50567, 170550, 170551,   7305, 170553, 170554,\n",
      "         80285, 170556, 170557,  47840, 170559, 170560,  27191, 170562, 170563,\n",
      "        140932, 170565, 170566,  97673, 170568, 170569,  36251, 170571, 170572,\n",
      "        158464, 170574, 170575,  28447, 170577, 170578,  90035, 170580, 170581,\n",
      "         35878, 170583, 170584,  38402, 170586, 170587, 150487, 170589, 170590,\n",
      "        147232, 170592, 170593,   4757, 170595, 170596,  65803, 170598, 170599,\n",
      "        136331, 170601, 170602,  36847, 170604, 170605, 129631, 170607, 170608,\n",
      "        125084, 170610, 170611,   3774, 170613, 170614,  93301, 170616, 170617,\n",
      "         70138, 170619, 170620, 168983, 170622, 170623,  39973, 170625, 170626,\n",
      "        167434, 170628, 170629,  48993, 170631, 170632, 134157, 170634, 170635,\n",
      "        142998, 170637, 170638, 142631, 170640, 170641, 136629, 170643, 170644,\n",
      "         70597, 170646, 170647,   5918, 170649, 170650, 164119, 170652, 170653,\n",
      "        146838, 170655, 170656, 129454, 170658, 170659,  56518, 170661, 170662,\n",
      "        104736, 170664, 170665,  54268, 170667, 170668, 127712, 170670, 170671,\n",
      "         51617, 170673, 170674,  87842, 170676, 170677, 131931, 170679, 170680,\n",
      "         75147, 170682, 170683,  78176, 170685, 170686, 124529, 170688, 170689,\n",
      "        144908, 170691, 170692, 152599, 170694, 170695, 109500, 170697, 170698,\n",
      "        152809, 170700, 170701, 123358, 170703, 170704, 113359, 170706, 170707,\n",
      "         63172, 170709, 170710,   8716, 170712, 170713,  47711, 170715, 170716,\n",
      "        153647, 170718, 170719,  78756, 170721, 170722,  85109, 170724, 170725,\n",
      "        120331, 170727, 170728,  54454, 170730, 170731, 157612, 170733, 170734,\n",
      "         15584, 170736, 170737,  48592, 170739, 170740, 128980, 170742, 170743,\n",
      "        118870, 170745, 170746, 145262, 170748, 170749,  94992, 170751, 170752,\n",
      "         28068, 170754, 170755,  58458, 170757, 170758,  41115, 170760, 170761,\n",
      "        103738, 170763, 170764,   2955, 170766, 170767,   6977, 170769, 170770,\n",
      "        108384, 170772, 170773, 168777, 170775, 170776,  70271, 170778, 170779,\n",
      "         89741, 170781, 170782, 131143, 170784, 170785, 109832, 170787, 170788,\n",
      "        126654, 170790, 170791,  92179, 170793, 170794, 100438, 170796, 170797,\n",
      "         58805, 170799, 170800, 103885, 170802, 170803, 147804, 170805, 170806,\n",
      "        113577, 170808, 170809,  21148, 170811, 170812,  75731, 170814, 170815,\n",
      "        100325, 170817, 170818,  68288, 170820, 170821,  79565, 170823, 170824,\n",
      "         28340, 170826, 170827,   8655, 170829, 170830,   4924, 170832, 170833,\n",
      "        156493, 170835, 170836, 148494, 170838, 170839,  32973, 170841, 170842,\n",
      "         44218, 170844, 170845, 151410, 170847, 170848,  65225, 170850, 170851,\n",
      "         69072, 170853, 170854,  49753, 170856, 170857,  72095, 170859, 170860,\n",
      "         15442, 170862, 170863, 113605, 170865, 170866,  69861, 170868, 170869,\n",
      "        108791, 170871, 170872, 123205, 170874, 170875, 121298, 170877, 170878,\n",
      "        162781, 170880, 170881,  82963, 170883, 170884, 159931, 170886, 170887,\n",
      "         34136, 170889, 170890,  10300, 170892, 170893,  39329, 170895, 170896,\n",
      "        157392, 170898, 170899, 107938, 170901, 170902,    494, 170904, 170905,\n",
      "         22166, 170907, 170908, 109920, 170910, 170911,  17234, 170913, 170914,\n",
      "         79404, 170916, 170917,  53596, 170919, 170920, 134072, 170922, 170923,\n",
      "         92397, 170925, 170926,   2688, 170928, 170929, 121386, 170931, 170932,\n",
      "        152968, 170934, 170935,  10262, 170937, 170938,  85337, 170940, 170941,\n",
      "        112591, 170943, 170944,   8951, 170946, 170947,  59919, 170949, 170950,\n",
      "         34034, 170952, 170953, 125602, 170955, 170956,  44639, 170958, 170959,\n",
      "         97368, 170961, 170962,   4367, 170964, 170965,  40594, 170967, 170968,\n",
      "         68981, 170970, 170971, 168198, 170973, 170974,  69316, 170976, 170977,\n",
      "         87326, 170979, 170980,  68237, 170982, 170983,  35037, 170985, 170986,\n",
      "        126689, 170988, 170989, 154968, 170991, 170992, 163394, 170994, 170995,\n",
      "        152383, 170997, 170998, 103946, 171000, 171001, 126176, 171003, 171004,\n",
      "         68477, 171006, 171007, 112778, 171009, 171010, 160670, 171012, 171013,\n",
      "        142487, 171015, 171016, 147511, 171018, 171019, 138188, 171021, 171022,\n",
      "         27413, 171024, 171025, 112906, 171027, 171028,  49367, 171030, 171031,\n",
      "         76346, 171033, 171034,  61945, 171036, 171037], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "len(poison_edge_index[0])\n",
    "poison_edge_index[0][len(train_edge_index[0]):]\n",
    "print(poison_edge_index[1][len(train_edge_index[0]):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171038"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poison_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:2')\n",
      "tensor([24, 24,  8, 30, 10, 28, 28, 37,  4, 24, 30, 24,  5, 24,  2,  5,  2,  8,\n",
      "         2, 24, 28,  2, 28, 16, 34, 10,  2, 24, 37, 24, 24,  2,  2, 38, 28,  6,\n",
      "        16,  2,  8, 24, 28,  2, 10, 16, 28, 28, 30,  2,  4,  4, 24, 13, 28, 28,\n",
      "         1, 28, 30, 16, 16, 16, 34, 14, 27, 16,  2, 16,  2, 28,  2,  4,  3, 24,\n",
      "         2, 24, 34, 34, 16,  1, 39, 30,  2, 38, 30, 34, 24,  2,  2, 16, 36, 23,\n",
      "         2,  2,  9,  4,  2, 19,  2, 34, 16, 28, 10,  2, 24, 24,  2,  2, 16, 19,\n",
      "         4, 16,  2, 24, 23,  2, 31, 22,  8,  2, 28, 24, 24, 24,  2,  9,  2,  2,\n",
      "        13, 28, 13, 28, 16, 28, 16,  2, 30, 30, 16, 24, 28,  6,  2, 28,  2, 28,\n",
      "        16, 16, 39,  2,  2, 34,  2, 24, 23,  0, 25, 16,  2, 30,  2, 28,  4, 16,\n",
      "         3, 24,  9,  9,  8, 24,  5, 13,  2, 24, 30, 31, 24,  2,  5, 19, 17, 16,\n",
      "        34, 19, 21, 19, 28, 10,  8, 16,  2,  8, 37, 24,  4, 37, 28, 16, 28,  4,\n",
      "         2, 26, 16, 30, 24, 19,  8,  2,  2,  4, 24,  2, 31, 19,  2, 16,  2, 30,\n",
      "         9, 24,  2,  2, 30, 34, 34, 10, 26,  2, 10,  2, 28, 24,  5, 38,  2, 19,\n",
      "        16, 10, 37, 18, 16, 30, 27, 26, 34, 28, 36, 31, 28, 31, 13, 16,  2,  2,\n",
      "         2, 16,  2, 22, 27, 23,  4, 16,  8,  5,  2,  2, 28,  4, 28, 27, 28, 30,\n",
      "        19,  2, 28, 28, 10,  2,  2, 13, 34, 30, 28, 24,  2, 24, 28, 30,  5, 30,\n",
      "         9, 19,  5, 16, 34,  2,  3,  2, 16, 26, 24, 30,  2, 16, 16, 28,  4, 28,\n",
      "         6, 28, 16, 31, 16, 24,  2, 28, 24,  6, 16, 30, 34,  3, 16, 28,  2,  2,\n",
      "        30, 28, 28,  2, 27,  5, 28, 28, 31, 24,  2,  2, 23, 13, 10, 27,  2, 24,\n",
      "        36,  2, 25, 28, 30, 34, 20, 28, 28, 26,  3, 16, 19,  2, 28,  4,  2, 36,\n",
      "        28,  2, 16, 28, 30, 16, 30, 23, 10,  4, 30, 34, 24,  7, 16, 30,  2, 10,\n",
      "        34, 31,  2,  8,  3,  2, 10, 28,  5, 16,  2, 16, 24, 20, 27, 19, 25, 28,\n",
      "         4, 24, 30,  2, 24, 28, 28,  8, 30, 24, 16, 16, 38, 39, 16,  2, 28,  2,\n",
      "        16, 30, 19, 23, 10, 16, 24, 16,  2, 16,  8,  2,  2, 34,  5,  2, 24,  2,\n",
      "        27, 27, 39, 16, 24, 28,  2, 16, 28, 24,  2, 24, 10, 16, 28, 34, 31, 16,\n",
      "        16,  2, 24, 31,  2,  3, 28, 16,  2, 20, 24, 36, 26,  5,  8,  8,  4, 19,\n",
      "        16, 24,  8, 24, 20, 27,  4,  2,  2, 16, 31,  2,  2,  3, 28,  3,  2, 34,\n",
      "        28,  4, 30, 31, 19, 28,  2, 13, 16, 28, 24,  2, 16,  4,  2,  2, 24,  2,\n",
      "        27, 16,  2, 36, 34, 34,  2, 16, 34, 24, 26,  8, 25, 24, 31,  2, 34, 27,\n",
      "        28, 15, 31, 39, 16, 19,  8, 37,  2,  8,  2, 10, 34,  2, 30, 16,  0, 37,\n",
      "         6, 24, 28, 10], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "## retrain a model on purified graph ##\n",
    "\n",
    "# labels for target poisoned node before purified #\n",
    "print(poison_labels[idx_attach])\n",
    "\n",
    "# purifying #\n",
    "for idx in index_of_less_robust:\n",
    "    poison_labels[bkd_tn_nodes[idx]]=data.y[bkd_tn_nodes[idx]]\n",
    "\n",
    "# labels for target poisoned node after purified #\n",
    "print(poison_labels[idx_attach])\n",
    "\n",
    "# retrain a model #\n",
    "test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "test_model.fit(poison_x,poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "## test model on purified graph (poisoned target node) ##\n",
    "clean_acc = test_model.test(poison_x,poison_edge_index, poison_edge_weights,poison_labels,idx_attach)\n",
    "\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR: 0.0620\n",
      "Flip ASR: 0.0394/16482 nodes\n",
      "CA: 0.6718\n"
     ]
    }
   ],
   "source": [
    "# add a trigger detector #\n",
    "# assumption: backdoor attack's success is based on trigger pattern #\n",
    "# 1. outlier, trigger different to each other #\n",
    "# 2. in distribution, trigger similar to each other #\n",
    "# # in case, model trained on clean graph learn attack pattern #\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "\n",
    "output, x = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "asr = train_attach_rate\n",
    "flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "print(\"CA: {:.4f}\".format(ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spurious",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
